"""
General CSV to RDF converter driven by a TOML configuration file.

Reads rdf_config.toml, processes all CSVs listed in the config,
and generates RDF triples accordingly.
"""

import argparse
from pathlib import Path
from typing import Union, Any
import logging
from wikidata_utils import extract_wd_id
import pandas as pd
import tomli
from tqdm import tqdm
from rdflib import Graph, URIRef, Literal, Namespace, XSD
from isodate.isoerror import ISO8601Error
from isodate.isodates import parse_date
from isodate.isodatetime import parse_datetime

# === Setup Logger ===
logger = logging.getLogger("csv_to_rdf")
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

# === Suppress rdflib Warnings ===
logging.getLogger("rdflib.term").setLevel(logging.ERROR)

def to_rdf_node(
    val: str,
    namespaces: dict,
    lang: str | None = None,
    datatype: str | None = None,
    prefix: str | None = None,
) -> Union[URIRef, Literal, None]:
    """Convert a value to the appropriate RDF node:

    A URIRef is returned if:
    1. The value is a Wikidata ID (e.g. Q3 or P1234)
    2. The value starts with a bound namespace URI or prefix

    None is returned if the value is empty or NaN.

    In all others cases, a Literal is returned.
    The Literal can have a language label or a datatype specified"""
    if not isinstance(val, str) or val == "":
        return None
    qid = extract_wd_id(val)
    if qid:
        return URIRef(f"{namespaces['wd']}{qid}")
    if val.startswith("http") and datatype not in ("xsd:anyURI", XSD.anyURI):
        return URIRef(val)
    # A prefix can be specified to expand the value to a full URI
    if prefix:
        return URIRef(f"{namespaces.get(prefix)}{val}")
    # Expand datatype if it is prefixed
    if datatype is not None and ":" in datatype:
        prefix, body = datatype.split(":", 1)
        ns_uri = namespaces.get(prefix)
        if ns_uri:
            datatype = f"{ns_uri}{body}"
    # Special logic for handling date datatype
    if datatype == XSD.date:
        try:
            # Validate the date string, and catch any exception that might occur
            return Literal(parse_date(val), datatype=XSD.date)
        except (ISO8601Error, ValueError):
            return Literal(val)  # Fallback to a plain literal if conversion fails
    # Special logic for handling datetime datatype
    if datatype == XSD.dateTime:
        try:
            # Validate the datetime string, and catch any exception that might occur
            return Literal(
                parse_datetime(val), datatype=XSD.dateTime
            )
        except (ISO8601Error, ValueError):
            return Literal(val)  # Fallback to a plain literal if conversion fails
    return Literal(str(val), lang=lang, datatype=datatype)


def to_predicate(val: str, namespaces: dict) -> URIRef:
    """
    Convert a property string to a predicate URIRef.

    A URIRef is returned if:
    1. The value is a Wikidata PID (e.g. P1234)
    2. The value starts with a bound namespace prefix or URI.

    This function raises a ValueError is none of the above conditions are met.
    It will never return Literal, since a predicate can not be a Literal.
    """
    # The function extracts both QID and PID
    wiki_id = extract_wd_id(val)
    if wiki_id and wiki_id.startswith("P"):
        return URIRef(f"{namespaces['wdt']}{wiki_id}")
    for prefix, uri in namespaces.items():
        if val.startswith(uri):
            # Only considered a value URI if it is in a bound namespace
            return URIRef(val)
        if val.startswith(prefix + ":"):
            # If the value starts with a prefix, expand to full URI
            return URIRef(f"{uri}{val.split(':', 1)[1]}")
    raise ValueError(
        f"Invalid property value: {val}. Expected a Wikidata ID or a valid prefixed URI."
    )


def rdf_process_predicates(
    toml_config: dict[str, dict[str, Any]],
) -> dict[str, dict[str, Any]]:
    """
    Resolve predicate strings to RDF URIs in a property config dict.

    Args:
    prop_config: A Python dictionary loaded from an RDF configuration TOML file,
                typically generated by generate_toml.py."

    Returns:
        A new config dict with all predicate strings replaced by resolved RDF URIs.
        The new config does not include the sections "general" and "namespaces"

    Raises:
        ValueError: On missing or invalid 'PRIMARY_KEY', invalid keys, or bad value types.
    """
    ns = toml_config["namespaces"]
    new_config = {}

    for file, file_schema in toml_config.items():
        # == Verify that each file has a PRIMARY_KEY ==
        if file in ("general", "namespaces"):
            new_config[file] = file_schema
            continue
        primary_key = file_schema.get("PRIMARY_KEY")
        if not primary_key:
            raise ValueError(f"Config [{file}]: missing 'PRIMARY_KEY' value.")
        if primary_key not in file_schema:
            raise ValueError(
                f"Config [{file}]: 'PRIMARY_KEY' does not point to an existing column."
            )

        new_file_schema = {}
        for col, col_schema in file_schema.items():
            if col == "PRIMARY_KEY":
                new_file_schema["PRIMARY_KEY"] = col_schema  # Keep PRIMARY_KEY as is
                continue
            if not col_schema:
                continue  # Skip empty fields
            if isinstance(col_schema, str):
                # Transform predicate to URIRef
                new_file_schema[col] = to_predicate(col_schema, ns)
            elif isinstance(col_schema, dict):
                # Complex config value
                for key in col_schema:
                    if key not in ("pred", "datatype", "lang", "subj", "if", "prefix"):
                        raise ValueError(
                            f"Config[{file}]: invalid key '{key}' in column '{col}'"
                        )
                # Verifying that datatype and lang are not both specified
                datatype = col_schema.get("datatype")
                lang = col_schema.get("lang")
                if datatype is not None and lang is not None:
                    raise ValueError(
                        f"Config[{file}]: cannot specify both datatype and lang for column '{col}'"
                    )
                # Transform predicate to URIRef, if it exists
                pred: str = col_schema.get("pred")
                new_file_schema[col] = {
                    **col_schema,
                    "pred": to_predicate(pred, ns),
                }
            else:
                raise ValueError(
                    f"Config[{file}]: invalid value type for column '{col}'. Expected a string or a dict, got {type(col_schema)}"
                )
        new_config[file] = new_file_schema
    return new_config


def rdf_transform_csv(
    df: pd.DataFrame, col_mapping: dict[str, str | dict], ns: dict
) -> pd.DataFrame:
    """Transform values of a DataFrame to RDF nodes based on provided column mappings"""
    cols_processed = set()
    subj_columns = []

    # Process columns based on property_columns
    for column, value in col_mapping.items():
        if column in cols_processed:
            continue
        if column == "PRIMARY_KEY":
            # Default processing for PRIMARY_KEY column
            if not col_mapping.get(value):
                df[value] = df[value].apply(lambda x: to_rdf_node(x, ns))
                cols_processed.add(value)
                continue
        elif isinstance(value, str) and value:
            # Processing all columns has a valid string value
            df[column] = df[column].apply(lambda x: to_rdf_node(x, ns))
            cols_processed.add(column)

        elif isinstance(value, dict) and value:
            df[column] = df[column].apply(
                lambda x, value=value: to_rdf_node(
                    x,
                    ns,
                    lang=value.get("lang"),
                    datatype=value.get("datatype"),
                    prefix=value.get("prefix"),
                )
            )
            cols_processed.add(column)

            if subj := value.get("subj"):
                subj_columns.append(subj)
    # Default process for columns only specified as subjects
    for column in subj_columns:
        if column not in cols_processed:
            df[column] = df[column].apply(lambda x: to_rdf_node(x, ns))
            cols_processed.add(column)
    return df


def fill_down_until_key(df: pd.DataFrame, primary_key: str) -> pd.DataFrame:
    """
    Fill down all columns in the DataFrame, but only until a non-empty value is encountered
    in the PRIMARY_KEY column. Rows after a non-empty PRIMARY_KEY value start a new block.

    Empty values are assumed to be either NaN or empty strings.
    """
    df = df.copy()
    # Any value not None becomes True
    mask = df[primary_key].notna()
    group = mask.cumsum()

    filled_df = df.groupby(group).ffill()
    # Restore original primary key column to avoid overwriting with filled values
    filled_df[primary_key] = df[primary_key]

    return filled_df


def build_rdf_graph(
    config: dict[str, dict],
) -> Graph:
    """
    Convert a CSV DataFrame to RDF triples and add them to the graph.

    column_mapping example:
    {
        "PRIMARY_KEY": "subject_column_name",
        "col1": "predicate1",
        "col2": {
            "pred": "ex:prop",
            "datatype": "xsd:string",
            "subj": "other_col",
            "if": "col3 == 'yes'"
        },
        ...
    }
    """
    try:
        rdf_ns = config["namespaces"]
        rel_inp_dir = Path(config["general"]["csv_folder"])
    except KeyError as e:
        raise ValueError(f" {config} is missing required key: {e}") from e
    # === Initialize RDF Graph ===
    graph = Graph()
    triple_counter = 0
    # === Bind Namespaces ===
    for prefix, ns in rdf_ns.items():
        graph.bind(prefix, Namespace(ns))

    # === Resolve CSV Folder Path ===
    script_dir = Path(__file__).parent.resolve()
    csv_folder = (script_dir / rel_inp_dir).resolve()
    # === Check for Test Mode ===
    # If test_mode is set to True, only sample up to 20 rows per CSV
    test_mode = config["general"].get("test_mode") is True
    if test_mode:
        logger.info("Running in test mode â€” sampling up to 20 rows per CSV file.")
    # === Opening csv files ===
    for csv_name, csv_schema in config.items():
        # "general" and "namespaces" are not CSV files
        if csv_name in ("general", "namespaces"):
            continue
        # table names do not have ".csv" extension
        csv_file = (csv_folder / csv_name).with_suffix(".csv")
        if not csv_file.exists():
            logger.warning("'%s' not found. Skipping.", csv_file)
            continue
        try:
            df = pd.read_csv(csv_file)
        except Exception as e:
            logger.error("Error reading '%s'. Skipping. %s", csv_file, e)
            continue
        if test_mode:
            df = df.sample(n=min(20, len(df)))
        logger.info("Processing %s...", csv_file.name)
        # === Convert entire csv to rdf node ===
        df = rdf_transform_csv(df, csv_schema, rdf_ns)
        # === Fill down records using PRIMARY_KEY as block marker ===
        primary_key = csv_schema["PRIMARY_KEY"]
        df = fill_down_until_key(df, primary_key)
        # === Make sure that Pandas does not store any NaN ===
        df = df.where(pd.notna(df), None)
        # === Iterate through all the rows of the CSV file ===
        for row in tqdm(df.itertuples(index=False), total=len(df)):
            primary_node = getattr(row, primary_key)
            for col, col_value in csv_schema.items():
                object_node = getattr(row, col, None)
                if object_node is None:
                    continue
                if isinstance(col_value, URIRef):
                    # simple config value
                    subject_node = primary_node
                    predicate = col_value
                elif isinstance(col_value, dict):
                    # complex config value
                    predicate = col_value["pred"]
                    if subj_col := col_value.get("subj"):
                        subject_node = getattr(row, subj_col, None)
                    else:
                        subject_node = primary_node
                    if not eval(
                        col_value.get("if", "True"),
                        {},
                        {"subj": subject_node, "obj": object_node},
                    ):
                        continue
                else:
                    continue
                if subject_node and predicate and object_node:
                    graph.add((subject_node, predicate, object_node))
                    triple_counter += 1
    # dynamically add the number of triples as a attribute of graph
    graph.count = triple_counter
    return graph


def main():

    # === Argument Parsing ===
    parser = argparse.ArgumentParser(
        description="Convert a CSV to RDF using a TOML configuration file."
    )
    parser.add_argument("config", type=str, help="Path to the TOML configuration file")
    args = parser.parse_args()
    config_path = Path(args.config)
    # === Load TOML config ===
    with open(config_path, "rb") as f:
        config = tomli.load(f)

    try:
        rel_out_dir = Path(config["general"]["rdf_output_folder"])
        ttl_name = config["general"]["name"]
    except KeyError as e:
        raise ValueError(f" {config} is missing required key: {e}") from e
    processed_config = rdf_process_predicates(config)
    rdf_graph = build_rdf_graph(processed_config)

    if rdf_graph:
        logger.info(
            "RDF graph built successfully. Serializing... (this may take a while)"
        )
        logger.info("RDF graph contains %d triples!", rdf_graph.count)
    # === Finding Output Directory ===
    script_dir = Path(__file__).parent.resolve()
    rdf_folder = (script_dir / rel_out_dir).resolve()
    ttl_path = (rdf_folder / ttl_name).with_suffix(".ttl")
    # === Serializing RDF Graph ===
    rdf_folder.mkdir(parents=True, exist_ok=True)
    rdf_graph.serialize(destination=ttl_path, format="turtle")
    logger.info("RDF conversion completed. Output saved to: %s", ttl_path.resolve())


if __name__ == "__main__":
    main()
