"""
General CSV to RDF converter driven by a TOML configuration file.

Reads rdf_config.toml, processes all CSVs listed in the config,
and generates RDF triples accordingly.
"""

import argparse
import time
from pathlib import Path
from typing import Union, Any
import logging
from wikidata_utils import extract_wd_id
import pandas as pd
import tomli
from tqdm import tqdm
from rdflib import Graph, URIRef, Literal, Namespace, XSD
from isodate.isoerror import ISO8601Error
from isodate.isodates import parse_date
from isodate.isodatetime import parse_datetime

# === Setup Logger ===
logger = logging.getLogger("csv_to_rdf")
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")

# === Suppress rdflib Warnings ===
logging.getLogger("rdflib.term").setLevel(logging.ERROR)


def to_rdf_node(
    val: str,
    namespaces: dict,
    lang: str | None = None,
    datatype: str | None = None,
    prefix: str | None = None,
) -> Union[URIRef, Literal, None]:
    """Convert a value to the appropriate RDF node:

    A URIRef is returned if:
    1. The value is a Wikidata ID (e.g. Q3 or P1234)
    2. The value starts with a bound namespace URI or prefix

    None is returned if the value is empty or NaN.

    In all others cases, a Literal is returned.
    The Literal can have a language label or a datatype specified"""
    if not isinstance(val, str) or val == "":
        return None
    qid = extract_wd_id(val)
    if qid:
        return URIRef(f"{namespaces['wd']}{qid}")
    if val.startswith("http") and datatype not in ("xsd:anyURI", XSD.anyURI):
        return URIRef(val)
    # A prefix can be specified to expand the value to a full URI
    if prefix:
        return URIRef(f"{namespaces.get(prefix)}{val}")
    # Expand datatype if it is prefixed
    if datatype is not None and ":" in datatype:
        prefix, body = datatype.split(":", 1)
        ns_uri = namespaces.get(prefix)
        if ns_uri:
            datatype = f"{ns_uri}{body}"
    # Special logic for handling date datatype
    if datatype == XSD.date:
        try:
            # Validate the date string, and catch any exception that might occur
            return Literal(parse_date(val), datatype=XSD.date)
        except (ISO8601Error, ValueError):
            return Literal(val)  # Fallback to a plain literal if conversion fails
    # Special logic for handling datetime datatype
    if datatype == XSD.dateTime:
        try:
            # Validate the datetime string, and catch any exception that might occur
            return Literal(parse_datetime(val), datatype=XSD.dateTime)
        except (ISO8601Error, ValueError):
            return Literal(val)  # Fallback to a plain literal if conversion fails
    return Literal(str(val), lang=lang, datatype=datatype)


def to_predicate(val: str, namespaces: dict) -> URIRef:
    """
    Convert a property string to a predicate URIRef.

    A URIRef is returned if:
    1. The value is a Wikidata PID (e.g. P1234)
    2. The value starts with a bound namespace prefix or URI.

    This function raises a ValueError is none of the above conditions are met.
    It will never return Literal, since a predicate can not be a Literal.
    """
    # The function extracts both QID and PID
    wiki_id = extract_wd_id(val)
    if wiki_id and wiki_id.startswith("P"):
        return URIRef(f"{namespaces['wdt']}{wiki_id}")
    for prefix, uri in namespaces.items():
        if val.startswith(uri):
            # Only considered a value URI if it is in a bound namespace
            return URIRef(val)
        if val.startswith(prefix + ":"):
            # If the value starts with a prefix, expand to full URI
            return URIRef(f"{uri}{val.split(':', 1)[1]}")
    raise ValueError(
        f"Invalid property value: {val}. Expected a Wikidata ID or a valid prefixed URI."
    )


def rdf_process_predicates(
    toml_config: dict[str, dict[str, Any]],
) -> dict[str, dict[str, Any]]:
    """
    Resolve predicate strings to RDF URIs in a property config dict.

    Args:
    prop_config: A Python dictionary loaded from an RDF configuration TOML file,
                typically generated by generate_toml.py."

    Returns:
        A new config dict with all predicate strings replaced by resolved RDF URIs.
        The new config does not include the sections "general" and "namespaces"
        Empty fields are removed from the config.

    Raises:
        ValueError: On missing or invalid 'PRIMARY_KEY', invalid keys, or bad value types.
    """
    ns = toml_config["namespaces"]
    new_config = {}

    for file, file_schema in toml_config.items():
        # == Verify that each file has a PRIMARY_KEY ==
        if file in ("general", "namespaces"):
            new_config[file] = file_schema
            continue
        primary_key = file_schema.get("PRIMARY_KEY")
        if not primary_key:
            raise ValueError(f"Config [{file}]: missing 'PRIMARY_KEY' value.")
        if primary_key not in file_schema:
            raise ValueError(
                f"Config [{file}]: 'PRIMARY_KEY' does not point to an existing column."
            )

        new_file_schema = {}
        for col, col_schema in file_schema.items():
            if col == "PRIMARY_KEY":
                new_file_schema["PRIMARY_KEY"] = col_schema  # Keep PRIMARY_KEY as is
                continue

            if isinstance(col_schema, str):
                # Skip empty fields
                if not col_schema:
                    continue
                # Transform predicate to URIRef
                new_file_schema[col] = to_predicate(col_schema, ns)
            elif isinstance(col_schema, dict):
                # Complex config value
                for key, val in col_schema.items():
                    if key not in ("pred", "datatype", "lang", "subj", "if", "prefix"):
                        raise ValueError(
                            f"Config[{file}]: invalid key '{key}' in column '{col}'"
                        )
                    if not isinstance(val, str):
                        raise ValueError(
                            f"Config[{file}]: '{key}' in column '{col}' must a string value"
                        )
                # Verifying that datatype and lang are not both specified
                datatype = col_schema.get("datatype")
                lang = col_schema.get("lang")
                if datatype and lang:
                    raise ValueError(
                        f"Config[{file}]: cannot specify both datatype and lang for column '{col}'"
                    )
                new_col_schema = {k: v for k, v in col_schema.items() if v != ""}
                # Skip empty dict
                if not new_col_schema:
                    continue
                # Transform predicate to URIRef, if it exists
                if "pred" in new_col_schema:
                    new_col_schema["pred"] = to_predicate(new_col_schema["pred"], ns)
                new_file_schema[col] = new_col_schema
            else:
                raise ValueError(
                    f"Config[{file}]: invalid value type for column '{col}'. Expected a string or a dict, got {type(col_schema)}"
                )
        if len(new_file_schema) > 1:
            new_config[file] = new_file_schema
    return new_config


def rdf_transform_csv(
    df: pd.DataFrame, col_mapping: dict[str, str | dict], ns: dict
) -> pd.DataFrame:
    """Transform values of a DataFrame to RDF nodes based on provided column mappings"""
    cols_processed = set()
    subj_columns = []

    # Process columns based on property_columns
    for column, value in col_mapping.items():
        if column in cols_processed:
            continue
        if column == "PRIMARY_KEY":
            # Default processing for PRIMARY_KEY column
            if not col_mapping.get(value):
                df[value] = df[value].apply(lambda x: to_rdf_node(x, ns))
                cols_processed.add(value)
                continue
        elif isinstance(value, str) and value:
            # Processing all columns has a valid string value
            df[column] = df[column].apply(lambda x: to_rdf_node(x, ns))
            cols_processed.add(column)

        elif isinstance(value, dict) and value:
            df[column] = df[column].apply(
                lambda x, value=value: to_rdf_node(
                    x,
                    ns,
                    lang=value.get("lang"),
                    datatype=value.get("datatype"),
                    prefix=value.get("prefix"),
                )
            )
            cols_processed.add(column)

            if subj := value.get("subj"):
                subj_columns.append(subj)
    # Default process for columns only specified as subjects
    for column in subj_columns:
        if column not in cols_processed:
            df[column] = df[column].apply(lambda x: to_rdf_node(x, ns))
            cols_processed.add(column)
    return df


def fill_down_until_key(df: pd.DataFrame, primary_key: str) -> pd.DataFrame:
    """
    Fill down all columns in the DataFrame, but only until a non-empty value is encountered
    in the PRIMARY_KEY column. Rows after a non-empty PRIMARY_KEY value start a new block.

    Empty values are assumed to be either NaN or empty strings.
    """
    df = df.copy()
    # Any value not None and not empty string becomes True
    mask = df[primary_key].notna() & (df[primary_key] != "") & (df[primary_key] != None)
    group = mask.cumsum()

    filled_df = df.groupby(group).ffill()
    # Restore original primary key column to avoid overwriting with filled values
    filled_df[primary_key] = df[primary_key]

    return filled_df


def build_rdf_graph(
    config: dict[str, dict],
) -> Graph:
    """
    Convert a CSV DataFrame to RDF triples and add them to the graph.

    column_mapping example:
    {
        "PRIMARY_KEY": "subject_column_name",
        "col1": "predicate1",
        "col2": {
            "pred": "ex:prop",
            "datatype": "xsd:string",
            "subj": "other_col",
            "if": "col3 == 'yes'"
        },
        ...
    }
    """
    try:
        rdf_ns = config["namespaces"]
        rel_inp_dir = Path(config["general"]["csv_folder"])
    except KeyError as e:
        raise ValueError(f" {config} is missing required key: {e}") from e
    # === Initialize RDF Graph ===
    graph = Graph()
    # === Bind Namespaces ===
    for prefix, ns in rdf_ns.items():
        graph.bind(prefix, Namespace(ns))

    # === Resolve CSV Folder Path ===
    script_dir = Path(__file__).parent.resolve()
    csv_folder = (script_dir / rel_inp_dir).resolve()
    # === Check for Test Mode ===
    # If test_mode is set to True, only sample up to 20 rows per CSV
    test_mode = config["general"].get("test_mode") is True
    if test_mode:
        logger.info("Running in test mode — sampling up to 20 rows per CSV file.")
    # === Opening csv files ===
    for csv_name, csv_schema in config.items():
        # "general" and "namespaces" are not CSV files
        if csv_name in ("general", "namespaces"):
            continue
        # table names do not have ".csv" extension
        csv_file = (csv_folder / csv_name).with_suffix(".csv")
        if not csv_file.exists():
            logger.warning("'%s' not found. Skipping.", csv_file)
            continue
        try:
            df = pd.read_csv(
                csv_file,
                dtype=str,
                keep_default_na=False,
                na_values=[
                    "",  # Empty string
                    " ",  # Space
                    "NA",  # Capitalized NA
                    "N/A",  # Common spreadsheet notation
                    "na",  # lowercase
                    "n/a",  # lowercase
                    "-",  # Often used to indicate "no data"
                    "--",  # Sometimes double-dash
                    "None",  # Pythonic
                    "none",  # lowercase variant
                    "NULL",  # SQL style
                    "null",  # lowercase
                    "NaN",  # Python/NumPy/Pandas
                    "nan",  # lowercase
                    "?",  # Occasionally used for unknowns
                ],
            )
        except Exception as e:
            logger.error("Error reading '%s'. Skipping. %s", csv_file, e)
            continue
        if test_mode:
            df = df.sample(n=min(20, len(df)))
        logger.info("Processing %s...", csv_file.name)
        # === Convert entire csv to rdf node ===
        df = rdf_transform_csv(df, csv_schema, rdf_ns)
        # === Fill down records using PRIMARY_KEY as block marker ===
        primary_key = csv_schema["PRIMARY_KEY"]
        df = fill_down_until_key(df, primary_key)
        # === Make sure that Pandas does not store any NaN ===
        df = df.where(pd.notna(df), None)
        # === Filter out columns that don't have predicates
        filtered_csv_schema = {
            k: v
            for k, v in csv_schema.items()
            # dict without "pred" were needed for rdf_transform_csv, but not for building the triple
            if k != "PRIMARY_KEY" and (not isinstance(v, dict) or "pred" in v)
        }
        # === Iterate through all the rows of the CSV file ===
        for _, row in tqdm(df.iterrows(), total=len(df)):
            primary_node = row[primary_key]
            for col, col_value in filtered_csv_schema.items():
                object_node = row.get(col, None)
                if object_node is None:
                    continue
                if isinstance(col_value, URIRef):
                    # simple config value
                    subject_node = primary_node
                    predicate = col_value
                elif isinstance(col_value, dict):
                    # complex config value
                    predicate = col_value["pred"]
                    if subj_col := col_value.get("subj"):
                        subject_node = row.get(subj_col, None)
                    else:
                        subject_node = primary_node
                    if not eval(
                        col_value.get("if", "True"),
                        {"URIRef": URIRef, "Literal": Literal, "None": None},
                        {"subj": subject_node, "obj": object_node, "row": row},
                    ):
                        continue
                else:
                    continue
                if subject_node and predicate and object_node:
                    try:
                        graph.add((subject_node, predicate, object_node))
                    except Exception as e:
                        raise ValueError(
                            f"Error adding triple ({subject_node}, {predicate}, {object_node}): {col}"
                        )

    # dynamically add the number of triples as a attribute of graph
    return graph


def main():

    start_time = time.time()
    # === Argument Parsing ===
    parser = argparse.ArgumentParser(
        description="Convert a CSV to RDF using a TOML configuration file."
    )
    parser.add_argument("config", type=str, help="Path to the TOML configuration file")
    args = parser.parse_args()
    config_path = Path(args.config)
    # === Load TOML config ===
    with open(config_path, "rb") as f:
        config = tomli.load(f)

    try:
        rel_out_dir = Path(config["general"]["rdf_output_folder"])
        ttl_name = config["general"]["name"]
    except KeyError as e:
        raise ValueError(f" {config} is missing required key: {e}") from e
    processed_config = rdf_process_predicates(config)
    rdf_graph = build_rdf_graph(processed_config)

    if rdf_graph:
        logger.info(
            "RDF graph built successfully"
        )
        logger.info("Serializing... (this may take a while)")

    # === Finding Output Directory ===
    script_dir = Path(__file__).parent.resolve()
    rdf_folder = (script_dir / rel_out_dir).resolve()
    ttl_path = (rdf_folder / ttl_name).with_suffix(".ttl")
    # === Serializing RDF Graph ===
    rdf_folder.mkdir(parents=True, exist_ok=True)
    rdf_graph.serialize(destination=ttl_path, format="turtle")
    logger.info("RDF conversion completed. Output saved to: %s", ttl_path.resolve())
    elapsed_time = time.time() - start_time
    logger.info("Script finished in %.2f seconds.", elapsed_time)
    # Each non-empty line in ttl (except prefix) is a triple
    non_empty_lines = sum(
    1 for line in ttl_path.open("r", encoding="utf-8")
    if line.strip() and not line.lstrip().startswith("@prefix"))
    logger.info("TTL file contains %d triples.", non_empty_lines)

if __name__ == "__main__":
    main()
