"""
Module: convert_to_rdf.py
This module converts line-delimited MusicBrainz JSON data into RDF Turtle format using the rdflib library.
It reads an input file where each line represents a MusicBrainz entity, processes the data to map various
properties to corresponding Wikidata properties, and serializes the resulting RDF graph to a Turtle file.
Key Features:
    - Reads and counts total lines in the input JSON file.
    - Infers the entity type based on the filename of the input.
    - Processes entity attributes including name, type, aliases, genres, and relationships.
    - Uses a mapping schema (MB_SCHEMA) to convert MusicBrainz entity relationships to corresponding Wikidata properties.
    - Processes data in chunks and utilizes asynchronous workers and multiprocessing for efficient data handling.
    - Merges RDF subgraphs generated by worker threads into a main RDF graph.
    - Serializes the main RDF graph to an output Turtle (.ttl) file specified via command line.
    - Provides progress feedback using tqdm for monitoring both file reading and graph merging processes.
Usage:
    python3 convert_to_rdf.py --input_folder <input_folder> --output_folder <output_folder>
    Where <input_folder> is a path to the folder containing line-delimited JSON files containing MusicBrainz data.
    and <output_folder> is the path to the folder to save the generated Turtle files.
    The script can also be run without arguments, in which case it will use default paths.
    The input folder should contain files named according to the entity type (e.g., artist.jsonl, release.jsonl).
    The output folder will contain the generated Turtle files named after the entity type.
    The script will create the output folder if it does not exist.
Exception Handling:
    - Skips any lines that fail JSON decoding or lack the necessary entity id.
This module is intended for converting MusicBrainz data to RDF format, facilitating integration with other linked data sources.
"""

import json
import sys
import os
import re
import argparse
import asyncio
from threading import Lock
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from tqdm import tqdm
from rdflib import Graph, URIRef, Literal, Namespace
from rdflib.namespace import RDF
import pandas as pd
import aiofiles


SCHEMA = Namespace("http://schema.org/")
WDT = Namespace("http://www.wikidata.org/prop/direct/")
WD = Namespace("http://www.wikidata.org/entity/")
MB = Namespace("https://musicbrainz.org/")
# Define MusicBrainz namespaces, to save space in exported RDF
MBAE = Namespace(f"{MB}area/")
MBAT = Namespace(f"{MB}artist/")
MBEV = Namespace(f"{MB}event/")
MBIN = Namespace(f"{MB}instrument/")
MBLA = Namespace(f"{MB}label/")
MBPL = Namespace(f"{MB}place/")
MBRC = Namespace(f"{MB}recording/")
MBRG = Namespace(f"{MB}release-group/")
MBRL = Namespace(f"{MB}release/")
MBSE = Namespace(f"{MB}series/")
MBWO = Namespace(f"{MB}work/")


class MappingSchema:
    """
    Class to hold the mapping schema for MusicBrainz to Wikidata.
    The first type is meant to be the type that you're pointing from,
    and the second type is the type that you're pointing to.
    This class handles wildcards, passing None as the first type will match any type,
    but will give priority to specific mappings.
    If the value for a mapping is None, that means that the mapping should not be processed.

    To retrieve method, use the syntax:
    `MB_SCHEMA[(pointing_from, pointing_to)]`
    where `pointing_from` is the type you're pointing from
    and `pointing_to` is the type you're pointing to.
    """

    def __init__(self, schema):
        """
        Initialize the MappingSchema with a given schema.
        The schema should be a dictionary where keys are tuples of (pointing_from, pointing_to)
        and values are the corresponding Wikidata property IDs.
        """
        self.schema = {}
        for types, mapping in schema.items():
            pointing_from, pointing_to = types
            if pointing_to not in self.schema:
                self.schema[pointing_to] = {}
            if pointing_from not in self.schema[pointing_to]:
                self.schema[pointing_to][pointing_from] = mapping

    def __getitem__(self, types):
        """Get the mapping for a given pair of types."""
        pointing_from, pointing_to = types
        if pointing_to in self.schema:
            if pointing_from in self.schema[pointing_to]:
                return self.schema[pointing_to][pointing_from]
            elif None in self.schema[pointing_to]:
                return self.schema[pointing_to][None]
        raise KeyError(f"No mapping found for types: {types}")

    def __contains__(self, t):
        """Check if a type is in the schema."""
        return t in self.schema

    def add(self, schema):
        """Add a new mapping to the schema."""
        for types, mapping in schema.items():
            pointing_from, pointing_to = types
            if pointing_to not in self.schema:
                self.schema[pointing_to] = {}
            if pointing_from not in self.schema[pointing_to]:
                self.schema[pointing_to][pointing_from] = mapping


# Define mapping from MusicBrainz to Wikidata, to pass to the above class
mapping_schema = {
    # Initialize wildcard mappings
    (None, "artist"): "P434",
    (None, "release-group"): "P436",
    (None, "release"): "P5813",
    (None, "recording"): "P4404",
    (None, "work"): "P435",
    (None, "label"): "P966",
    (None, "area"): "P982",
    (None, "place"): "P1004",
    (None, "event"): "P6423",
    (None, "series"): "P1407",
    (None, "instrument"): "P1330",
    (None, "genre"): "P8052",
    (None, "url"): "P2888",
    # Specific mappings for areas
    # Specific mappings for artists
    # Specific mappings for events
    # Specific mappings for instruments
    ("instrument", "instrument"): "P279",
    # Specific mappings for labels
    # Specific mappings for places
    # Specific mappings for recordings
    ("recording", "artist"): "P175",
    # Specific mappings for release groups
    ("release-group", "artist"): "P175",
    # Specific mappings for releases
    ("release", "artist"): "P175",
    # Specific mappings for series
    # Specific mappings for works
    # Specific mappings for genres
}

MB_SCHEMA = MappingSchema(mapping_schema)

CHUNK_SIZE = 500  # Adjustable chunk size
# Max number of chunk processing threads to run simultaneously
MAX_SIMULTANEOUS_CHUNK_WORKERS = 3
# Max number of processes to run simultaneously
MAX_PROCESSES = min(MAX_SIMULTANEOUS_CHUNK_WORKERS, os.cpu_count() or 1)
MAX_CHUNKS_IN_MEMORY = 150  # Max number of chunks to keep in memory at once
MAX_SUBGRAPHS_IN_MEMORY = 150  # Max number of subgraphs to keep in memory at once

REPROCESSING = False  # Set to True if you want to reprocess entity types that are already present in the output folder

# Entity types that do not have types, so we don't need to process them
ENTITIES_WITHOUT_TYPES = [
    "recording",
    "release-group",
    "release",
]


def matched_wikidata(field: str) -> bool:
    """Check if the field is a matched Wikidata ID."""
    return re.match(r"^Q\d+", field) is not None


def process_line(data, entity_type, mb_schema, g, mb_entity_types, type_mapping):
    """Process a single line of JSON data and add it to the RDF graph."""
    entity_id = data.get("id")
    if not entity_id:
        return

    # Create subject URI
    subject_uri = URIRef(f"https://musicbrainz.org/{entity_type}/{entity_id}")

    # Process name
    if "name" in data:
        g.add((subject_uri, URIRef(f"{WDT}P2561"), Literal(data["name"])))

    # Process type
    if "type" in data and (converted_type := type_mapping.get(data["type"])):
        # If the type is a Wikidata ID, use it directly
        if matched_wikidata(converted_type):
            g.add((subject_uri, RDF.type, URIRef(f"{WD}{converted_type}")))
        else:
            g.add((subject_uri, RDF.type, Literal(data["type"])))

    # Process release group
    if "release-group" in data:
        g.add(
            (
                subject_uri,
                URIRef(f"{WDT}{mb_schema[(entity_type, 'release-group')]}"),
                URIRef(
                    f"https://musicbrainz.org/release-group/{data['release-group']["id"]}"
                ),
            )
        )

    # Process artists
    for artist in data.get("artist-credit", []):
        g.add(
            (
                subject_uri,
                URIRef(f"{WDT}{mb_schema[(entity_type, 'artist')]}"),
                URIRef(f"https://musicbrainz.org/artist/{artist['artist']['id']}"),
            )
        )

    # Process aliases
    for alias in data.get("aliases", []):
        if alias_name := alias.get("name"):
            try:
                g.add(
                    (
                        subject_uri,
                        URIRef(f"{WDT}P4970"),
                        Literal(
                            alias_name,
                            lang=alias.get("locale", "none"),
                        ),
                    )
                )
            except ValueError:  # If the language isn't in the standard RDF languages
                g.add(
                    (
                        subject_uri,
                        URIRef(f"{WDT}P4970"),
                        Literal(alias_name),
                    )
                )

    # Process genres
    for genre in data.get("genres", []):
        if genre_id := genre.get("id"):
            genre_uri = URIRef(f"https://musicbrainz.org/genre/{genre_id}")
            g.add(
                (
                    subject_uri,
                    URIRef(f"{WDT}{MB_SCHEMA[(entity_type, 'genre')]}"),
                    genre_uri,
                )
            )

    # Process relationships
    for relation in data.get("relations", []):
        if not (rel_type := relation.get("target-type")):
            continue

        target_uri = None
        for key in relation:
            if key in mb_entity_types:
                if target_id := relation[key].get("id"):
                    target_uri = URIRef(f"https://musicbrainz.org/{key}/{target_id}")
                    break
            elif key == "url":
                if url_resource := relation[key].get("resource"):
                    url_resource = re.sub(
                        r"^https://www.wikidata.org/wiki/Q(\d+)$",
                        f"{WD}Q\\g<1>",
                        url_resource,
                    )
                    target_uri = URIRef(url_resource)
                    break

        if target_uri and rel_type in mb_schema:
            pred_uri = URIRef(f"{WDT}{MB_SCHEMA[(entity_type, rel_type)]}")
            g.add((subject_uri, pred_uri, target_uri))


def process_chunk(chunk, entity_type, mb_schema, mb_entity_types, type_mapping):
    """Process a chunk of data and add it to the subgraph."""
    g = Graph()
    for i, line in enumerate(chunk):
        try:
            data = json.loads(line.strip())
            process_line(
                data,
                entity_type,
                mb_schema,
                g,
                mb_entity_types,
                type_mapping,
            )
        except json.JSONDecodeError:
            continue
        finally:
            chunk[i] = None  # Clear the processed line to free memory
    return g


async def subgraph_worker(
    chunk_queue,
    subgraph_queue,
    entity_type,
    mb_schema,
    type_mapping,
    chunk_bar,
    chunk_bar_lock,
    executor,
):
    """
    Worker function to process data chunks.
    This function runs in a separate process to speed up the processing.
    """
    loop = asyncio.get_event_loop()
    try:
        while True:
            chunk = await chunk_queue.get()

            MB_ENTITY_TYPES = {
                "artist",
                "release",
                "recording",
                "label",
                "work",
                "area",
                "genre",
                "event",
                "place",
                "series",
                "instrument",
            }

            # Process the chunk in a separate process to speed up the processing
            g = await loop.run_in_executor(
                executor,
                process_chunk,
                chunk,
                entity_type,
                mb_schema,
                MB_ENTITY_TYPES,
                type_mapping,
            )

            await subgraph_queue.put(g)  # Add the subgraph to the queue
            chunk_queue.task_done()
            with chunk_bar_lock:
                chunk_bar.update(1)
    except asyncio.CancelledError:
        pass


def merge_subgraph(main_graph, subgraph):
    """Merge a subgraph into the main graph."""
    for s, p, o in subgraph:
        try:
            main_graph.add((s, p, o))
        except Exception as e:
            print(f"Error adding triple ({s}, {p}, {o}) to main graph: {e}")
            continue
    main_graph.commit()  # Commit the changes to the main graph


async def graph_worker(subgraph_queue, main_graph, subgraph_bar):
    """Worker function to merge subgraphs into the main graph."""
    loop = asyncio.get_event_loop()
    sigint = False
    try:
        while True:
            subgraph = await subgraph_queue.get()

            # Do this in a separate thread to avoid blocking the event loop
            await loop.run_in_executor(None, merge_subgraph, main_graph, subgraph)

            subgraph_queue.task_done()
            subgraph_bar.update(1)
    except asyncio.CancelledError:
        sigint = True
    except Exception as e:
        print(f"Error merging subgraph: {e}")
    finally:
        if not sigint:
            while not subgraph_queue.empty():
                subgraph = await subgraph_queue.get()
                await loop.run_in_executor(None, merge_subgraph, main_graph, subgraph)
                subgraph_queue.task_done()
                subgraph_bar.update(1)


async def get_final_graph(entity_type, input_file, namespaces, type_mapping):
    """Main function to process the input file and return the final RDF graph."""
    with open(input_file, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)
    total_chunks = (
        total_lines // CHUNK_SIZE + 1
        if total_lines % CHUNK_SIZE != 0
        else total_lines // CHUNK_SIZE
    )
    print(f"Total lines in {input_file}: {total_lines}")
    print(f"Total number of chunks: {total_chunks}")
    print(f"Processing {input_file}...")

    # Create task queue
    chunk_queue = asyncio.Queue(MAX_CHUNKS_IN_MEMORY)
    subgraph_queue = asyncio.Queue(MAX_SUBGRAPHS_IN_MEMORY)

    # Create the progress bars
    file_bar = tqdm(total=total_lines, desc="Processing lines", position=0)
    chunk_bar = tqdm(total=total_chunks, desc="Processing chunks", position=1)
    subgraph_bar = tqdm(total=total_chunks, desc="Merging subgraphs", position=2)
    # Lock for thread safety for the chunk bar because each worker can update it
    chunk_bar_lock = Lock()

    main_graph = Graph("Oxigraph")
    main_graph.open("./store", create=True)
    for prefix, ns in namespaces.items():
        main_graph.bind(prefix, ns)

    with ProcessPoolExecutor(max_workers=MAX_PROCESSES) as executor:
        try:
            subgraph_workers = [
                asyncio.create_task(
                    subgraph_worker(
                        chunk_queue,
                        subgraph_queue,
                        entity_type,
                        MB_SCHEMA,
                        type_mapping,
                        chunk_bar,
                        chunk_bar_lock,
                        executor,
                    )
                )
                for _ in range(MAX_SIMULTANEOUS_CHUNK_WORKERS)
            ]
            merge_worker = asyncio.create_task(
                graph_worker(subgraph_queue, main_graph, subgraph_bar)
            )

            # Read file and split into chunks
            async with aiofiles.open(input_file, "r", encoding="utf-8") as f:
                chunk = []
                async for line in f:
                    chunk.append(line)
                    if len(chunk) >= CHUNK_SIZE:
                        await chunk_queue.put(chunk)
                        chunk = []
                    file_bar.update(1)
                if chunk:  # Process the remaining last chunk
                    await chunk_queue.put(chunk)

            await chunk_queue.join()  # Wait for all chunks to be processed

            for worker in subgraph_workers:
                worker.cancel()

            await subgraph_queue.join()  # Wait for all subgraphs to be processed

            merge_worker.cancel()

            await asyncio.gather(*subgraph_workers, merge_worker)

            file_bar.close()
            with chunk_bar_lock:
                chunk_bar.close()
            subgraph_bar.close()
        except KeyboardInterrupt:
            executor.shutdown(wait=False, cancel_futures=True)

    return main_graph


def main(args):
    """Main function to handle command line arguments and process the input file."""
    # Parse command line arguments
    input_file = args.input_file
    entity_type = Path(args.input_file).stem  # Get entity type from filename
    type_file = args.type_file
    if type_file:
        # Read the types from the type file
        types = pd.read_csv(type_file, encoding="utf-8")
        type_mapping = dict(zip(types["type"], types["type_@id"]))
    else:
        type_mapping = {}

    # Configure output directory
    output_folder = Path(args.output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)
    output_file = output_folder / f"{entity_type}.ttl"

    # Initialize namespaces
    namespaces = {
        "schema": SCHEMA,
        "wdt": WDT,
        "wd": WD,
        "mb": MB,
        "mbae": MBAE,
        "mbat": MBAT,
        "mbev": MBEV,
        "mbin": MBIN,
        "mbla": MBLA,
        "mbpl": MBPL,
        "mbrc": MBRC,
        "mbrg": MBRG,
        "mbrl": MBRL,
        "mbse": MBSE,
        "mbwo": MBWO,
    }

    main_graph = asyncio.run(get_final_graph(entity_type, input_file, namespaces, type_mapping))

    # Save the final result
    print(f"Saving RDF data to: {output_file}")
    with open(output_file, "wb") as f:
        main_graph.serialize(f, format="turtle", encoding="utf-8")
    print(f"Successfully saved RDF data to: {output_file}")
    main_graph.close()

    # Fully delete the store
    for root, dirs, files in os.walk("./store", topdown=False):
        for file in files:
            os.remove(os.path.join(root, file))
        for name in dirs:
            os.rmdir(os.path.join(root, name))
    os.rmdir("./store")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Convert MusicBrainz JSON data in a folder to RDF Turtle format."
    )
    parser.add_argument(
        "--input_folder",
        default="../../data/musicbrainz/raw/extracted_jsonl/mbdump",
        help="Path to the folder containing line-delimited MusicBrainz JSON files.",
    )
    parser.add_argument(
        "--type_folder",
        default="../../data/musicbrainz/raw/types/",
        help="Path to the folder containing MusicBrainz entity types reconciled against Wikidata.",
    )
    parser.add_argument(
        "--output_folder",
        default="../../data/musicbrainz/rdf/",
        help="Directory where the output Turtle files will be saved.",
    )
    args = parser.parse_args()

    input_folder = Path(args.input_folder)
    if not input_folder.is_dir():
        print(f"{input_folder} is not a valid directory.")
        sys.exit(1)

    bad_files = []
    if Path(args.output_folder).exists() and not REPROCESSING:
        output_folder = Path(args.output_folder)
        for file in output_folder.iterdir():
            if file.is_file():
                bad_files.append(file.stem)

    for input_file in input_folder.iterdir():
        if not input_file.is_file() or not str(input_file).endswith(".jsonl"):
            continue
        if not REPROCESSING and input_file.stem in bad_files:
            print(f"Skipping {input_file} as it is already processed.")
            continue
        type_file = Path(args.type_folder) / f"{input_file.stem}-types-csv.csv"
        if not type_file.exists() or not type_file.is_file():
            type_file = None
        print(f"Processing file: {input_file}")
        # Create a new namespace for the current file using its stem as entity type
        sub_args = argparse.Namespace(
            input_file=str(input_file), type_file=type_file, output_folder=args.output_folder
        )
        main(sub_args)
