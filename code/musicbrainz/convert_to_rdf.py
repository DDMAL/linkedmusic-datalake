"""
Module: convert_to_rdf.py
This module converts line-delimited MusicBrainz JSON data into RDF Turtle format using the rdflib library.
It reads an input file where each line represents a MusicBrainz entity, processes the data to map various
properties to corresponding Wikidata properties, and serializes the resulting RDF graph to a Turtle file.

Key Features:

    - Reads and counts total lines in the input JSON file.
    - Infers the entity type based on the filename of the input.
    - Processes entity attributes including name, type, aliases, genres, and relationships.
    - Uses a mapping schema (MB_SCHEMA) to convert MusicBrainz entity relationships to corresponding Wikidata properties.
    - Processes data in chunks and utilizes asynchronous workers and multiprocessing for efficient data handling.
    - Merges RDF subgraphs generated by worker threads into a main RDF graph.
    - Serializes the main RDF graph to an output Turtle (.ttl) file specified via command line.
    - Provides progress feedback using tqdm for monitoring both file reading and graph merging processes.

Usage:
    python3 convert_to_rdf.py --input_folder <input_folder> --reconciled_folder <reconciled_folder>
        --config_folder <config_folder> --output_folder <output_folder>

    Where <input_folder> is a path to the folder containing line-delimited JSON files containing MusicBrainz data,
    <reconciled_folder> is the path to the folder containing reconciled mappings for types and keys,
    <config_folder> is the path to the folder containing configuration files for the schema and mappings,
    and <output_folder> is the path to the folder to save the generated Turtle files.
    The script can also be run without arguments, in which case it will use default paths.
    The input folder should contain files named according to the entity type (e.g., artist.jsonl, release.jsonl).
    The output folder will contain the generated Turtle files named after the entity type.
    The script will create the output folder if it does not exist.

Exception Handling:

    - Skips any lines that fail JSON decoding or lack the necessary entity id.
    - Handles missing or malformed data gracefully, logging errors to the terminal without crashing the script.

This module is intended for converting MusicBrainz data to RDF format, facilitating integration with other linked data sources.
"""

import json
import sys
import os
import re
import argparse
import asyncio
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from isodate.isoerror import ISO8601Error
from isodate.isodates import parse_date
from isodate.isodatetime import parse_datetime
from tqdm import tqdm
from rdflib import Graph, URIRef, Literal, Namespace
from rdflib.namespace import XSD
import pandas as pd
import aiofiles
from url_regex import DATABASES_REGEX

# Define namespaces
SCHEMA = Namespace("http://schema.org/")
WDT = Namespace("http://www.wikidata.org/prop/direct/")
WD = Namespace("http://www.wikidata.org/entity/")
# This namespace is to encode coordinates like Wikidata does
GEO = Namespace("http://www.opengis.net/ont/geosparql#")
MB = Namespace("https://musicbrainz.org/")
# Define MusicBrainz namespaces, to save space in exported RDF
MBAE = Namespace(f"{MB}area/")
MBAT = Namespace(f"{MB}artist/")
MBCD = Namespace(f"{MB}cdtoc/")
MBEV = Namespace(f"{MB}event/")
MBGE = Namespace(f"{MB}genre/")
MBIN = Namespace(f"{MB}instrument/")
MBLA = Namespace(f"{MB}label/")
MBPL = Namespace(f"{MB}place/")
MBRC = Namespace(f"{MB}recording/")
MBRG = Namespace(f"{MB}release-group/")
MBRL = Namespace(f"{MB}release/")
MBSE = Namespace(f"{MB}series/")
MBWO = Namespace(f"{MB}work/")


class MappingSchema:
    """
    Class to hold the mapping schema for MusicBrainz to Wikidata.
    The first type is meant to be the type that you're pointing from,
    and the second type is the type that you're pointing to.
    This class handles wildcards, passing None as the first type will match any type,
    but will give priority to specific mappings.
    Any mappings passed as a string will be converted to a URIRef
    with the Wikidata namespace (WDT).

    To retrieve values, use the syntax:
    `MB_SCHEMA[(pointing_from, pointing_to)]`
    where `pointing_from` is the type you're pointing from
    and `pointing_to` is the type you're pointing to.

    Additionally, you can use the `to_dict_for_type` method to convert the schema
    to a dictionary for a specific entity type.
    This will simplify calls to the schema to avoid having to pass the pointing_from type.
    """

    def __init__(self, schema):
        """
        Initialize the MappingSchema with a given schema.
        The schema should be a dictionary where keys are tuples of (pointing_from, pointing_to)
        and values are the corresponding Wikidata property IDs.
        pointing_from can be a single type or an iterable of types.
        pointing_to should always be a single type.
        If the value is a string, it will be converted to a URIRef with the Wikidata namespace (WDT).
        """
        self.schema = {}
        for types, mapping in schema.items():
            if not isinstance(mapping, URIRef):
                mapping = URIRef(f"{WDT}{mapping}")
            pointing_from, pointing_to = types
            if pointing_to not in self.schema:
                self.schema[pointing_to] = {}
            if pointing_from and not isinstance(pointing_from, str):  # Handle iterables
                for pf in pointing_from:
                    self.schema[pointing_to][pf] = mapping
            else:
                self.schema[pointing_to][pointing_from] = mapping

    def __getitem__(self, types):
        """Get the mapping for a given pair of types."""
        pointing_from, pointing_to = types
        if pointing_to in self.schema:
            if pointing_from in self.schema[pointing_to]:
                return self.schema[pointing_to][pointing_from]
            elif None in self.schema[pointing_to]:
                return self.schema[pointing_to][None]
        raise KeyError(f"No mapping found for types: {types}")

    def __contains__(self, t):
        """Check if a type is in the schema."""
        return t in self.schema

    def __bool__(self):
        """Check if the schema is not empty."""
        return bool(self.schema)

    def add(self, schema):
        """Add an additional mapping to the schema, with the same properties and behaviour as init."""
        for types, mapping in schema.items():
            if not isinstance(mapping, URIRef):
                mapping = URIRef(f"{WDT}{mapping}")
            pointing_from, pointing_to = types
            if pointing_to not in self.schema:
                self.schema[pointing_to] = {}
            if pointing_from and not isinstance(pointing_from, str):  # Handle iterables
                for pf in pointing_from:
                    self.schema[pointing_to][pf] = mapping
            else:
                self.schema[pointing_to][pointing_from] = mapping

    def to_dict_for_type(self, entity_type):
        """
        Convert the schema to a dictionary for a specific entity type.
        This will simplify calls to the schema to avoid having to pass the pointing_from type.
        This will return a dictionary where keys are the pointing_to types
        and values are the corresponding URIRef.
        """
        to_return = {}
        for pointing_to, mappings in self.schema.items():
            for pointing_from, mapping in mappings.items():
                if pointing_from == entity_type or (
                    pointing_from is None and pointing_to not in to_return
                ):
                    to_return[pointing_to] = mapping
        return to_return

    def add_from_formatted_dict(self, formatted_dict):
        """
        Add mappings from a formatted dictionary to the schema.
        The formatted dictionary should have the same layout as the internal schema,
        with the exception that values are full URLs instead of URIRefs.
        Additionally, if keys corresponding to `pointing_from` are the string value
        "null", they will be converted to None.
        This is a convenience method to add mappings from a dictionary
        that was the internal schema dictionary dumped to a JSON file.
        """
        self.add(
            {
                (key if key != "null" else None, k): URIRef(val)
                for k, v in formatted_dict.items()
                for key, val in v.items()
            }
        )


MB_SCHEMA = MappingSchema({})

# Initialize the relationship and attribute mappings
RELATIONSHIP_MAPPING = {}
ATTRIBUTE_MAPPING = {}
RECONCILIATION_MAPPING = {}

CHUNK_SIZE = 500  # Adjustable chunk size
# Max number of chunk processing threads to run simultaneously
MAX_SIMULTANEOUS_CHUNK_WORKERS = 3
# Max number of processes to run simultaneously
MAX_PROCESSES = min(MAX_SIMULTANEOUS_CHUNK_WORKERS, os.cpu_count() or 1)
MAX_CHUNKS_IN_MEMORY = 120  # Max number of chunks to keep in memory at once
MAX_SUBGRAPHS_IN_MEMORY = 120  # Max number of subgraphs to keep in memory at once

# If the input file is bigger (in bytes) than this, it will use Oxigraph to store the graph
# Otherwise it will use rdflib's in-memory graph
GRAPH_STORE_CUTOFF = 1000000000

REPROCESSING = False  # Set to True if you want to reprocess entity types that are already present in the output folder

# Entity types that do not have types, so we don't need to process them
ENTITIES_WITHOUT_TYPES = [
    "recording",
    "release",
]

# This one is seperated because I need to convert from /wiki/... to /entity/...
WIKIDATA_REGEX = re.compile(r"^https?:\/\/www\.wikidata\.org\/wiki\/Q\d+$")


def matched_wikidata(field: str) -> bool:
    """Check if the field is a matched Wikidata ID."""
    return re.match(r"^Q\d+", field) is not None


def convert_date(date_str: str) -> Literal:
    """
    Convert a date string to an RDF Literal with XSD date datatype.
    If the date string is not in a valid format, it returns a plain Literal.
    """
    try:
        # Validate the date string, and catch any exception that might occur
        return Literal(parse_date(date_str), datatype=XSD.date)
    except (ISO8601Error, ValueError):
        return Literal(date_str)  # Fallback to a plain literal if conversion fails


def convert_datetime(date_str: str, time_str: str) -> Literal:
    """
    Convert a date and time string to an RDF Literal with XSD dateTime datatype.
    If the date or time string is not in a valid format, it returns a plain Literal.
    """
    try:
        # Validate the datetime string, and catch any exception that might occur
        return Literal(
            parse_datetime(f"{date_str}T{time_str}:00"), datatype=XSD.dateTime
        )
    except (ISO8601Error, ValueError):
        return Literal(date_str)  # Fallback to a plain literal if conversion fails


def process_line(
    data,
    entity_type,
    mb_schema,
    relationship_mapping,
    reconciled_mapping,
    attribute_mapping,
    g,
):
    """Process a single line of JSON data and add it to the RDF graph."""
    entity_id = data.get("id")
    if not entity_id:
        return

    # Create subject URI
    subject_uri = URIRef(f"https://musicbrainz.org/{entity_type}/{entity_id}")

    # Process id
    g.add((subject_uri, mb_schema[f"{entity_type}-id"], Literal(entity_id)))

    # Process name
    if name := data.get("name"):
        g.add((subject_uri, mb_schema["name"], Literal(name)))

    # Process type
    for t in data.get("secondary-types", []) + [
        data.get("primary-type"),
        data.get("type"),
    ]:
        if t and (converted_type := reconciled_mapping.get(t)):
            # If the type is a Wikidata ID, use it directly
            if matched_wikidata(converted_type):
                g.add((subject_uri, mb_schema["type"], URIRef(f"{WD}{converted_type}")))
            else:
                g.add((subject_uri, mb_schema["type"], Literal(t)))

    # Process address
    if address := data.get("address"):
        g.add(
            (
                subject_uri,
                mb_schema["address"],
                Literal(address),
            )
        )

    # Process aliases
    for alias in data.get("aliases", []):
        if alias_name := alias.get("name"):
            try:
                g.add(
                    (
                        subject_uri,
                        mb_schema["alias"],
                        Literal(
                            alias_name,
                            lang=alias.get("locale", "none"),
                        ),
                    )
                )
            except ValueError:  # If the language isn't in the standard RDF languages
                g.add(
                    (
                        subject_uri,
                        mb_schema["alias"],
                        Literal(alias_name),
                    )
                )

    # Process area
    if area := data.get("area"):
        if area_id := area.get("id"):
            g.add(
                (
                    subject_uri,
                    mb_schema["area"],
                    URIRef(f"https://musicbrainz.org/area/{area_id}"),
                )
            )

    # Process artists
    for artist in data.get("artist-credit", []):
        if artist_id := artist.get("artist", {}).get("id"):
            g.add(
                (
                    subject_uri,
                    mb_schema["artist"],
                    URIRef(f"https://musicbrainz.org/artist/{artist_id}"),
                )
            )

    # Process ASIN
    if asin := data.get("asin"):
        g.add(
            (
                subject_uri,
                mb_schema["asin"],
                Literal(asin),
            )
        )

    # Process attributes
    for attribute in data.get("attributes", []):
        if (attribute_type := attribute.get("type")) and (
            attribute_value := attribute.get("value")
        ):
            if pred := attribute_mapping.get(attribute_type):
                if (
                    attribute_type == "Key"
                    and (key_map := reconciled_mapping.get(attribute_value))
                    and matched_wikidata(key_map)
                ):
                    # If the attribute is the key and it is a Wikidata ID, use it directly
                    attribute_value = URIRef(f"{WD}{key_map}")
                else:
                    attribute_value = Literal(attribute_value)

                g.add(
                    (
                        subject_uri,
                        pred,
                        attribute_value,
                    )
                )

    # Process barcode
    if barcode := data.get("barcode"):
        g.add(
            (
                subject_uri,
                mb_schema["barcode"],
                Literal(barcode),
            )
        )

    # Process begin area
    if begin_area := data.get("begin_area"):
        if begin_area_id := begin_area.get("id"):
            g.add(
                (
                    subject_uri,
                    (
                        mb_schema["begin-area-person"]
                        if data["type"] == "Person"
                        else mb_schema["begin-area"]
                    ),
                    URIRef(f"https://musicbrainz.org/area/{begin_area_id}"),
                )
            )

    # Process coordinates
    if coordinates := data.get("coordinates"):
        if (lat := coordinates.get("latitude")) and (
            lon := coordinates.get("longitude")
        ):
            g.add(
                (
                    subject_uri,
                    mb_schema["coordinates"],
                    Literal(f"Point({lat} {lon})", datatype=GEO["wktLiteral"]),
                )
            )

    # Process date
    if date := data.get("date"):
        g.add((subject_uri, mb_schema["date"], convert_date(date)))

    # Process end area
    if data.get("end_area"):
        if end_area_id := data["end_area"].get("id"):
            g.add(
                (
                    subject_uri,
                    (
                        mb_schema["end-area-person"]
                        if data["type"] == "Person"
                        else mb_schema["end-area"]
                    ),
                    URIRef(f"https://musicbrainz.org/area/{end_area_id}"),
                )
            )

    # Process first release date
    if first_release_date := data.get("first-release-date"):
        g.add(
            (
                subject_uri,
                mb_schema["first-release-date"],
                convert_date(first_release_date),
            )
        )

    # Process gender
    if gender := data.get("gender"):
        if (gender_map := reconciled_mapping.get(gender)) and matched_wikidata(gender_map):
            gender = URIRef(f"{WD}{gender_map}")
        else:
            gender = Literal(gender)
        g.add(
            (
                subject_uri,
                mb_schema["gender"],
                gender,
            )
        )

    # Process genres
    for genre in data.get("genres", []):
        if genre_id := genre.get("id"):
            genre_uri = URIRef(f"https://musicbrainz.org/genre/{genre_id}")
            g.add(
                (
                    subject_uri,
                    mb_schema["genre"],
                    genre_uri,
                )
            )

    # Process IPIs
    for ipi in data.get("ipis", []):
        g.add(
            (
                subject_uri,
                mb_schema["ipi"],
                Literal(ipi),
            )
        )

    # Process ISNIs
    for isni in data.get("isnis", []):
        g.add(
            (
                subject_uri,
                mb_schema["isni"],
                Literal(isni),
            )
        )

    # Process ISWCs
    for iswc in data.get("iswcs", []):
        g.add(
            (
                subject_uri,
                mb_schema["iswc"],
                Literal(iswc),
            )
        )

    # Process label code
    if label_code := data.get("label-code"):
        g.add(
            (
                subject_uri,
                mb_schema["labelcode"],
                Literal(label_code),
            )
        )

    # Process labels
    for label in data.get("label-info", []):
        label = label.get("label")
        if label and (label_id := label.get("id")):
            g.add(
                (
                    subject_uri,
                    mb_schema["label"],
                    URIRef(f"https://musicbrainz.org/label/{label_id}"),
                )
            )

    # Process length
    if length := data.get("length"):
        try:
            length_seconds = int(length) // 1000  # Convert milliseconds to seconds
            g.add(
                (
                    subject_uri,
                    mb_schema["length"],
                    Literal(str(length_seconds), datatype=XSD.decimal),
                )
            )
        except (ValueError, TypeError):
            # Fallback to encoding as a regular string if conversion fails
            g.add(
                (
                    subject_uri,
                    mb_schema["length"],
                    Literal(length),
                )
            )

    # Process lifespan
    if lifespan := data.get("life-span"):
        if begin_date := lifespan.get("begin"):
            g.add(
                (
                    subject_uri,
                    (
                        mb_schema["begin-date-person"]
                        if data["type"] == "Person"
                        else mb_schema["begin-date"]
                    ),
                    convert_date(begin_date),
                )
            )
        if end_date := lifespan.get("end"):
            g.add(
                (
                    subject_uri,
                    (
                        mb_schema["end-date-person"]
                        if data["type"] == "Person"
                        else mb_schema["end-date"]
                    ),
                    convert_date(end_date),
                )
            )

    # Process media
    for media in data.get("media", []):
        for disc in media.get("discs", []):
            if disc_id := disc.get("id"):
                g.add(
                    (
                        subject_uri,
                        mb_schema["cdtoc"],
                        URIRef(f"https://musicbrainz.org/cdtoc/{disc_id}"),
                    )
                )

        for track in media.get("tracks", []):
            recording = track.get("recording")
            if recording and (recording_id := recording.get("id")):
                g.add(
                    (
                        subject_uri,
                        mb_schema["recording"],
                        URIRef(f"https://musicbrainz.org/recording/{recording_id}"),
                    )
                )

    # Process relationships
    for relation in data.get("relations", []):
        if not (target_type := relation.get("target-type")) or not (
            rel_type := relation.get("type")
        ):
            continue

        target = None
        pred_uri = None
        if target_type == "url" and (url := relation.get("url", {}).get("resource")):
            pred_uri = mb_schema["url"]
            if WIKIDATA_REGEX.match(url):
                # Convert Wikidata URL to URIRef
                target = URIRef(
                    re.sub(
                        r"^https?:\/\/www\.wikidata\.org\/wiki\/(Q\d+)",
                        f"{WD}\\g<1>",
                        url,
                    )
                )
            else:
                # Check if the URL matches any of the known databases
                for db, regex in DATABASES_REGEX.items():
                    if match := regex.match(url):
                        target = Literal(str(match.group(1)))
                        pred_uri = mb_schema[db]
                        break
                else:
                    # If no match, treat it as a generic URL
                    target = Literal(url)

        if not target:
            # Handle homogeneous relations
            if target_type.replace("_", "-") == entity_type and (
                rel_direction := relation.get("direction")
            ):
                rel_type += rel_direction
            pred_uri = relationship_mapping.get(target_type, {}).get(rel_type)

            if target_id := relation.get(target_type, {}).get("id"):
                # If the target is a MusicBrainz entity, create a URIRef
                target = URIRef(f"{MB}{target_type}/{target_id}")

        if target and pred_uri:
            g.add((subject_uri, pred_uri, target))

    # Process release events
    for event in data.get("release-events", []):
        area = event.get("area")
        if area and (area_id := area.get("id")):
            g.add(
                (
                    subject_uri,
                    mb_schema["area"],
                    URIRef(f"https://musicbrainz.org/area/{area_id}"),
                )
            )

    # Process release group
    if release_group := data.get("release-group"):
        if release_group_id := release_group.get("id"):
            g.add(
                (
                    subject_uri,
                    mb_schema["release-group"],
                    URIRef(f"https://musicbrainz.org/release-group/{release_group_id}"),
                )
            )

    # Process time
    if (time := data.get("time")) and (date := data.get("life-span", {}).get("begin")):
        # If both date and time are present, convert them to a datetime literal
        g.add(
            (
                subject_uri,
                mb_schema["time"],
                convert_datetime(date, time),
            )
        )

    # Process title
    if title := data.get("title"):
        g.add((subject_uri, mb_schema["title"], Literal(title)))


def process_chunk(
    chunk,
    entity_type,
    mb_schema,
    relationship_mapping,
    reconciled_mapping,
    attribute_mapping,
):
    """Process a chunk of data and add it to the subgraph."""
    g = Graph()
    for i, line in enumerate(chunk):
        try:
            data = json.loads(line.strip())
            process_line(
                data,
                entity_type,
                mb_schema,
                relationship_mapping,
                reconciled_mapping,
                attribute_mapping,
                g,
            )
        except json.JSONDecodeError:
            continue
        except (KeyError, AttributeError) as e:
            with tqdm.get_lock():
                tqdm.write(f"{type(e).__name__} in line {i} of chunk: {e}")
            continue
        except Exception as e:
            with tqdm.get_lock():
                tqdm.write(f"Unexpected {type(e).__name__} in line {i} of chunk: {e}")
            continue
        finally:
            chunk[i] = None  # Clear the processed line to free memory
    return g


async def subgraph_worker(
    chunk_queue,
    subgraph_queue,
    entity_type,
    mb_schema,
    relationship_mapping,
    reconciled_mapping,
    chunk_bar,
    executor,
):
    """
    Worker function to process data chunks.
    This function runs in a separate process to speed up the processing.
    """
    loop = asyncio.get_event_loop()
    try:
        while True:
            chunk = await chunk_queue.get()

            # Process the chunk in a separate process to speed up the processing
            g = await asyncio.gather(
                loop.run_in_executor(
                    executor,
                    process_chunk,
                    chunk,
                    entity_type,
                    mb_schema,
                    relationship_mapping,
                    reconciled_mapping,
                    ATTRIBUTE_MAPPING,
                ),
                return_exceptions=True,
            )
            g = g[0]

            # Handle any exceptions raised by the process
            if isinstance(g, Exception):
                chunk_queue.task_done()
                with tqdm.get_lock():
                    tqdm.write(f"Error processing chunk: {type(g).__name__}: {g}")
                    chunk_bar.update(1)
                continue

            await subgraph_queue.put(g)  # Add the subgraph to the queue
            chunk_queue.task_done()
            with tqdm.get_lock():
                chunk_bar.update(1)
    except asyncio.CancelledError:
        pass


def merge_subgraph(main_graph, subgraph):
    """Merge a subgraph into the main graph."""
    for s, p, o in subgraph:
        try:
            main_graph.add((s, p, o))
        except Exception as e:
            with tqdm.get_lock():
                tqdm.write(
                    f"Error adding triple ({s}, {p}, {o}) to main graph: {type(e).__name__}: {e}"
                )
            continue
    main_graph.commit()  # Commit the changes to the main graph


async def graph_worker(subgraph_queue, main_graph, subgraph_bar):
    """Worker function to merge subgraphs into the main graph."""
    loop = asyncio.get_event_loop()
    sigint = False
    try:
        while True:
            subgraph = await subgraph_queue.get()

            # Do this in a separate thread to avoid blocking the event loop
            await loop.run_in_executor(None, merge_subgraph, main_graph, subgraph)

            subgraph_queue.task_done()
            with tqdm.get_lock():
                subgraph_bar.update(1)
    except asyncio.CancelledError:
        sigint = True
    except Exception as e:
        with tqdm.get_lock():
            tqdm.write(f"Error merging subgraph: {type(e).__name__}: {e}")
    finally:
        if not sigint:
            while not subgraph_queue.empty():
                subgraph = await subgraph_queue.get()
                await loop.run_in_executor(None, merge_subgraph, main_graph, subgraph)
                subgraph_queue.task_done()
                with tqdm.get_lock():
                    subgraph_bar.update(1)


async def get_final_graph(
    entity_type, input_file, namespaces, reconciled_mapping, relationship_mapping
):
    """Main function to process the input file and return the final RDF graph."""
    with open(input_file, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)
    total_chunks = (
        total_lines // CHUNK_SIZE + 1
        if total_lines % CHUNK_SIZE != 0
        else total_lines // CHUNK_SIZE
    )
    print(f"Total lines in {input_file}: {total_lines}")
    print(f"Total number of chunks: {total_chunks}")
    print(f"Processing {input_file}...")

    if os.path.getsize(input_file) > GRAPH_STORE_CUTOFF:
        main_graph = Graph("Oxigraph")
        main_graph.open("./store", create=True)
    else:
        print(f"{input_file} is small enough to use an in-memory graph.")
        main_graph = Graph()

    for prefix, ns in namespaces.items():
        main_graph.bind(prefix, ns)

    # Create task queue
    chunk_queue = asyncio.Queue(MAX_CHUNKS_IN_MEMORY)
    subgraph_queue = asyncio.Queue(MAX_SUBGRAPHS_IN_MEMORY)

    # Create the progress bars
    file_bar = tqdm(total=total_lines, desc="Processing lines", position=0)
    chunk_bar = tqdm(total=total_chunks, desc="Processing chunks", position=1)
    subgraph_bar = tqdm(total=total_chunks, desc="Merging subgraphs", position=2)

    with ProcessPoolExecutor(max_workers=MAX_PROCESSES) as executor:
        try:
            subgraph_workers = [
                asyncio.create_task(
                    subgraph_worker(
                        chunk_queue,
                        subgraph_queue,
                        entity_type,
                        MB_SCHEMA.to_dict_for_type(entity_type),
                        relationship_mapping,
                        reconciled_mapping,
                        chunk_bar,
                        executor,
                    )
                )
                for _ in range(MAX_SIMULTANEOUS_CHUNK_WORKERS)
            ]
            merge_worker = asyncio.create_task(
                graph_worker(subgraph_queue, main_graph, subgraph_bar)
            )

            # Read file and split into chunks
            async with aiofiles.open(input_file, "r", encoding="utf-8") as f:
                chunk = []
                async for line in f:
                    chunk.append(line)
                    if len(chunk) >= CHUNK_SIZE:
                        await chunk_queue.put(chunk)
                        chunk = []
                    with tqdm.get_lock():
                        file_bar.update(1)
                if chunk:  # Process the remaining last chunk
                    await chunk_queue.put(chunk)

            with tqdm.get_lock():
                file_bar.refresh()

            await chunk_queue.join()  # Wait for all chunks to be processed

            for worker in subgraph_workers:
                worker.cancel()

            await asyncio.gather(*subgraph_workers)

            with tqdm.get_lock():
                chunk_bar.refresh()

            await subgraph_queue.join()  # Wait for all subgraphs to be processed

            merge_worker.cancel()

            await asyncio.gather(merge_worker)

            file_bar.close()
            chunk_bar.close()
            subgraph_bar.close()
        except KeyboardInterrupt:
            executor.shutdown(wait=False, cancel_futures=True)

    return main_graph


def main(args):
    """Main function to handle command line arguments and process the input file."""
    # Parse command line arguments
    input_file = args.input_file
    entity_type = Path(args.input_file).stem  # Get entity type from filename
    type_file = Path(args.type_file) if args.type_file else None
    if type_file:
        # Read the types from the type file
        types = pd.read_csv(type_file, encoding="utf-8")
        reconciled_mapping = dict(zip(types["type"], types["type_@id"]))
    else:
        reconciled_mapping = {}
    reconciled_mapping.update(RECONCILIATION_MAPPING)

    # Configure output directory
    output_folder = Path(args.output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)
    output_file = output_folder / f"{entity_type}.ttl"

    # Initialize namespaces
    namespaces = {
        "schema": SCHEMA,
        "wdt": WDT,
        "wd": WD,
        "geo": GEO,
        "mb": MB,
        "mbae": MBAE,
        "mbat": MBAT,
        "mbcd": MBCD,
        "mbev": MBEV,
        "mbge": MBGE,
        "mbin": MBIN,
        "mbla": MBLA,
        "mbpl": MBPL,
        "mbrc": MBRC,
        "mbrg": MBRG,
        "mbrl": MBRL,
        "mbse": MBSE,
        "mbwo": MBWO,
    }

    main_graph = asyncio.run(
        get_final_graph(
            entity_type,
            input_file,
            namespaces,
            reconciled_mapping,
            RELATIONSHIP_MAPPING.get(entity_type, {}),
        )
    )

    # Save the final result
    print(f"Saving RDF data to: {output_file}")
    with open(output_file, "wb") as f:
        main_graph.serialize(f, format="turtle", encoding="utf-8")
    print(f"Successfully saved RDF data to: {output_file}")
    main_graph.close()

    # Fully delete the store
    if os.path.exists("./store"):
        for root, dirs, files in os.walk("./store", topdown=False):
            for file in files:
                os.remove(os.path.join(root, file))
            for name in dirs:
                os.rmdir(os.path.join(root, name))
        os.rmdir("./store")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Convert MusicBrainz JSON data in a folder to RDF Turtle format."
    )
    parser.add_argument(
        "--input_folder",
        default="../../data/musicbrainz/raw/extracted_jsonl/mbdump",
        help="Path to the folder containing line-delimited MusicBrainz JSON files.",
    )
    parser.add_argument(
        "--reconciled_folder",
        default="../../data/musicbrainz/raw/reconciled/",
        help="Path to the folder containing data reconciled against Wikidata.",
    )
    parser.add_argument(
        "--config_folder",
        default="../../doc/musicbrainz/rdf_conversion_config/",
        help="Path to the folder containing MusicBrainz RDF conversion configuration files (the property, relationship, and attribute mapping files).",
    )
    parser.add_argument(
        "--output_folder",
        default="../../data/musicbrainz/rdf/",
        help="Directory where the output Turtle files will be saved.",
    )
    args = parser.parse_args()

    input_folder = Path(args.input_folder)
    if not input_folder.is_dir():
        print(f"{input_folder} is not a valid directory.")
        sys.exit(1)

    config_folder = Path(args.config_folder)
    if not config_folder.is_dir():
        print(f"{config_folder} is not a valid directory.")
        sys.exit(1)

    with open(config_folder / "mappings.json", "r", encoding="utf-8") as fi:
        MB_SCHEMA.add_from_formatted_dict(json.load(fi))
    if not MB_SCHEMA:
        print("No mappings found in the configuration file.")
        sys.exit(1)

    with open(config_folder / "attribute_mapping.json", "r", encoding="utf-8") as fi:
        ATTRIBUTE_MAPPING = json.load(fi)
    if not ATTRIBUTE_MAPPING:
        print("No attribute mapping found in the configuration file.")
        sys.exit(1)

    for k, v in ATTRIBUTE_MAPPING.items():
        ATTRIBUTE_MAPPING[k] = URIRef(f"{WDT}{v}") if v else None

    with open(config_folder / "relations.json", "r", encoding="utf-8") as fi:
        RELATIONSHIP_MAPPING = json.load(fi)
    if not RELATIONSHIP_MAPPING:
        print("No relationship mapping found in the configuration file.")
        sys.exit(1)

    # Convert the property ID strings into URIRefs
    for mapping in RELATIONSHIP_MAPPING.values():
        if not mapping:
            continue
        for values in mapping.values():
            if not values:
                continue
            for k, v in values.items():
                if v is not None:
                    values[k] = WDT[v]

    keys_file_path = Path(args.reconciled_folder) / "keys-csv.csv"
    if keys_file_path.is_file():
        with open(keys_file_path, "r", encoding="utf-8") as fi:
            keys = pd.read_csv(fi, encoding="utf-8")
            RECONCILIATION_MAPPING.update(dict(zip(keys["key"], keys["key_@id"])))
    
    genders_file_path = Path(args.reconciled_folder) / "genders-csv.csv"
    if genders_file_path.is_file():
        with open(genders_file_path, "r", encoding="utf-8") as fi:
            genders = pd.read_csv(fi, encoding="utf-8")
            RECONCILIATION_MAPPING.update(dict(zip(genders["gender"], genders["gender_@id"])))

    bad_files = []
    if Path(args.output_folder).exists() and not REPROCESSING:
        output_folder = Path(args.output_folder)
        for file in output_folder.iterdir():
            if file.is_file():
                bad_files.append(file.stem)

    for input_file in input_folder.iterdir():
        if not input_file.is_file() or not str(input_file).endswith(".jsonl"):
            continue
        if not REPROCESSING and input_file.stem in bad_files:
            print(f"Skipping {input_file} as it is already processed.")
            continue
        type_file = None
        if input_file.stem not in ENTITIES_WITHOUT_TYPES:
            type_file_path = (
                Path(args.reconciled_folder) / f"{input_file.stem}-types-csv.csv"
            )
            if type_file_path.exists() and type_file_path.is_file():
                type_file = str(type_file_path)
        print(f"Processing file: {input_file}")

        # Create a new namespace for the current file using its stem as entity type
        sub_args = argparse.Namespace(
            input_file=str(input_file),
            type_file=type_file,
            output_folder=args.output_folder,
        )
        main(sub_args)
