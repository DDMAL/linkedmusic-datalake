"""
Module: convert_to_rdf.py
This module converts line-delimited MusicBrainz JSON data into RDF Turtle format using the rdflib library.
It reads an input file where each line represents a MusicBrainz entity, processes the data to map various
properties to corresponding Wikidata properties, and serializes the resulting RDF graph to a Turtle file.
Key Features:
    - Reads and counts total lines in the input JSON file.
    - Infers the entity type based on the filename of the input.
    - Processes entity attributes including name, type, aliases, genres, and relationships.
    - Uses a mapping schema (MB_SCHEMA) to convert MusicBrainz entity relationships to corresponding Wikidata properties.
    - Processes data in chunks and utilizes asynchronous workers and multiprocessing for efficient data handling.
    - Merges RDF subgraphs generated by worker threads into a main RDF graph.
    - Serializes the main RDF graph to an output Turtle (.ttl) file specified via command line.
    - Provides progress feedback using tqdm for monitoring both file reading and graph merging processes.
Usage:
    python3 convert_to_rdf.py --input_folder <input_folder> --output_folder <output_folder>
    Where <input_folder> is a path to the folder containing line-delimited JSON files containing MusicBrainz data.
    and <output_folder> is the path to the folder to save the generated Turtle files.
    The script can also be run without arguments, in which case it will use default paths.
    The input folder should contain files named according to the entity type (e.g., artist.jsonl, release.jsonl).
    The output folder will contain the generated Turtle files named after the entity type.
    The script will create the output folder if it does not exist.
Exception Handling:
    - Skips any lines that fail JSON decoding or lack the necessary entity id.
This module is intended for converting MusicBrainz data to RDF format, facilitating integration with other linked data sources.
"""

import json
import sys
import os
import re
import argparse
import asyncio
from threading import Lock
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from tqdm import tqdm
from rdflib import Graph, URIRef, BNode, Literal, Namespace
from rdflib.namespace import RDF
import aiofiles


SCHEMA = Namespace("http://schema.org/")
WDT = Namespace("http://www.wikidata.org/prop/direct/")
WD = Namespace("http://www.wikidata.org/entity/")
MB = Namespace("https://musicbrainz.org/")
# Define MusicBrainz namespaces, to save space in exported RDF
MBAE = Namespace(f"{MB}area/")
MBAT = Namespace(f"{MB}artist/")
MBEV = Namespace(f"{MB}event/")
MBIN = Namespace(f"{MB}instrument/")
MBLA = Namespace(f"{MB}label/")
MBPL = Namespace(f"{MB}place/")
MBRC = Namespace(f"{MB}recording/")
MBRG = Namespace(f"{MB}release-group/")
MBRL = Namespace(f"{MB}release/")
MBSE = Namespace(f"{MB}series/")
MBWO = Namespace(f"{MB}work/")

# Define mapping from MusicBrainz to Wikidata
MB_SCHEMA = {
    "artist": "P434",
    "release-group": "P436",
    "release": "P5813",
    "recording": "P4404",
    "work": "P435",
    "label": "P966",
    "area": "P982",
    "place": "P1004",
    "event": "P6423",
    "series": "P1407",
    "instrument": "P1330",
    "genre": "P8052",
    "url": "P2888",
}

CHUNK_SIZE = 500  # Adjustable chunk size
MAX_SIMULTANEOUS_CHUNK_WORKERS = 3  # Max number of chunk processing threads to run simultaneously
MAX_PROCESSES = min(MAX_SIMULTANEOUS_CHUNK_WORKERS, os.cpu_count())  # Max number of processes to run simultaneously
MAX_CHUNKS_IN_MEMORY = 150  # Max number of chunks to keep in memory at once
MAX_SUBGRAPHS_IN_MEMORY = 150  # Max number of subgraphs to keep in memory at once

REPROCESSING = False  # Set to True if you want to reprocess entity types that are already present in the output folder


def process_line(data, entity_type, mb_schema, g, mb_entity_types):
    """Process a single line of JSON data and add it to the RDF graph."""
    entity_id = data.get("id")
    if not entity_id:
        return

    # Create subject URI
    subject_uri = URIRef(f"https://musicbrainz.org/{entity_type}/{entity_id}")

    # Process name
    if "name" in data:
        g.add((subject_uri, URIRef(f"{WDT}P2561"), Literal(data["name"])))

    # Process type
    if "type" in data:
        g.add((subject_uri, RDF.type, Literal(data["type"])))
    
    # Process release group
    if "release-group" in data:
        g.add((subject_uri, URIRef(f"{WDT}{mb_schema['release-group']}"), URIRef(f"https://musicbrainz.org/release-group/{data['release-group']["id"]}")))

    # Process aliases
    if "aliases" in data:
        for alias in data["aliases"]:
            if alias_name := alias.get("name"):
                blank_node = BNode()
                g.add((subject_uri, URIRef(f"{WDT}P4970"), blank_node))
                try:
                    g.add(
                        (
                            blank_node,
                            URIRef(f"{WDT}P2561"),
                            Literal(
                                alias_name,
                                lang=alias.get("locale", "none"),
                            ),
                        )
                    )
                except ValueError:
                    g.add(
                        (
                            blank_node,
                            URIRef(f"{WDT}P2561"),
                            Literal(alias_name),
                        )
                    )
                    # Process alias language
                    if "locale" in alias:
                        g.add(
                            (
                                blank_node,
                                URIRef(f"{WDT}P1412"),
                                Literal(alias["locale"]),
                            )
                        )

    # Process genres
    if "genres" in data:
        for genre in data["genres"]:
            if genre_id := genre.get("id"):
                genre_uri = URIRef(f"https://musicbrainz.org/genre/{genre_id}")
                g.add(
                    (
                        subject_uri,
                        URIRef(f"{WDT}{MB_SCHEMA['genre']}"),
                        genre_uri,
                    )
                )

    # Process relationships
    if "relations" in data:
        for relation in data["relations"]:
            if not (rel_type := relation.get("target-type")):
                continue

            target_uri = None
            for key in relation:
                if key in mb_entity_types:
                    if target_id := relation[key].get("id"):
                        target_uri = URIRef(
                            f"https://musicbrainz.org/{key}/{target_id}"
                        )
                        break
                elif key == "url":
                    if url_resource := relation[key].get("resource"):
                        url_resource = re.sub(r"^https://www.wikidata.org/wiki/Q(\d+)$", f"{WD}Q\\g<1>", url_resource)
                        target_uri = URIRef(url_resource)
                        break

            if target_uri and rel_type in mb_schema:
                pred_uri = URIRef(f"{WDT}{MB_SCHEMA[rel_type]}")
                g.add((subject_uri, pred_uri, target_uri))


def process_chunk(chunk, entity_type, mb_schema, mb_entity_types):
    """Process a chunk of data and add it to the subgraph."""
    g = Graph()
    for i, line in enumerate(chunk):
        try:
            data = json.loads(line.strip())
            process_line(
                data,
                entity_type,
                mb_schema,
                g,
                mb_entity_types,
            )
        except json.JSONDecodeError:
            continue
        finally:
            chunk[i] = None  # Clear the processed line to free memory
    return g


async def subgraph_worker(
    chunk_queue,
    subgraph_queue,
    entity_type,
    mb_schema,
    chunk_bar,
    chunk_bar_lock,
    executor,
):
    """
    Worker function to process data chunks.
    This function runs in a separate process to speed up the processing.
    """
    loop = asyncio.get_event_loop()
    try:
        while True:
            chunk = await chunk_queue.get()

            MB_ENTITY_TYPES = {
                "artist",
                "release",
                "recording",
                "label",
                "work",
                "area",
                "genre",
                "event",
                "place",
                "series",
                "instrument",
            }

            # Process the chunk in a separate process to speed up the processing
            g = await loop.run_in_executor(
                executor,
                process_chunk,
                chunk,
                entity_type,
                mb_schema,
                MB_ENTITY_TYPES,
            )

            await subgraph_queue.put(g)  # Add the subgraph to the queue
            chunk_queue.task_done()
            with chunk_bar_lock:
                chunk_bar.update(1)
    except asyncio.CancelledError:
        pass


def merge_subgraph(main_graph, subgraph):
    """Merge a subgraph into the main graph."""
    for s, p, o in subgraph:
        try:
            main_graph.add((s, p, o))
        except Exception as e:
            print(f"Error adding triple ({s}, {p}, {o}) to main graph: {e}")
            continue
    main_graph.commit()  # Commit the changes to the main graph


async def graph_worker(subgraph_queue, main_graph, subgraph_bar):
    """Worker function to merge subgraphs into the main graph."""
    loop = asyncio.get_event_loop()
    sigint = False
    try:
        while True:
            subgraph = await subgraph_queue.get()

            # Do this in a separate thread to avoid blocking the event loop
            await loop.run_in_executor(None, merge_subgraph, main_graph, subgraph)

            subgraph_queue.task_done()
            subgraph_bar.update(1)
    except asyncio.CancelledError:
        sigint = True
    except Exception as e:
        print(f"Error merging subgraph: {e}")
    finally:
        if not sigint:
            while not subgraph_queue.empty():
                subgraph = await subgraph_queue.get()
                await loop.run_in_executor(None, merge_subgraph, main_graph, subgraph)
                subgraph_queue.task_done()
                subgraph_bar.update(1)


async def get_final_graph(entity_type, input_file, namespaces):
    """Main function to process the input file and return the final RDF graph."""
    with open(input_file, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)
    total_chunks = (
        total_lines // CHUNK_SIZE + 1
        if total_lines % CHUNK_SIZE != 0
        else total_lines // CHUNK_SIZE
    )
    print(f"Total lines in {input_file}: {total_lines}")
    print(f"Total number of chunks: {total_chunks}")
    print(f"Processing {input_file}...")

    # Create task queue
    chunk_queue = asyncio.Queue(MAX_CHUNKS_IN_MEMORY)
    subgraph_queue = asyncio.Queue(MAX_SUBGRAPHS_IN_MEMORY)

    # Create the progress bars
    file_bar = tqdm(total=total_lines, desc="Processing lines", position=0)
    chunk_bar = tqdm(total=total_chunks, desc="Processing chunks", position=1)
    subgraph_bar = tqdm(total=total_chunks, desc="Merging subgraphs", position=2)
    # Lock for thread safety for the chunk bar because each worker can update it
    chunk_bar_lock = Lock()

    main_graph = Graph("Oxigraph")
    main_graph.open("./store", create=True)
    for prefix, ns in namespaces.items():
        main_graph.bind(prefix, ns)

    with ProcessPoolExecutor(max_workers=MAX_PROCESSES) as executor:
        try:
            subgraph_workers = [
                asyncio.create_task(
                    subgraph_worker(
                        chunk_queue,
                        subgraph_queue,
                        entity_type,
                        MB_SCHEMA,
                        chunk_bar,
                        chunk_bar_lock,
                        executor,
                    )
                )
                for _ in range(MAX_SIMULTANEOUS_CHUNK_WORKERS)
            ]
            merge_worker = asyncio.create_task(
                graph_worker(subgraph_queue, main_graph, subgraph_bar)
            )

            # Read file and split into chunks
            async with aiofiles.open(input_file, "r", encoding="utf-8") as f:
                chunk = []
                async for line in f:
                    chunk.append(line)
                    if len(chunk) >= CHUNK_SIZE:
                        await chunk_queue.put(chunk)
                        chunk = []
                    file_bar.update(1)
                if chunk:  # Process the remaining last chunk
                    await chunk_queue.put(chunk)

            await chunk_queue.join()  # Wait for all chunks to be processed

            for worker in subgraph_workers:
                worker.cancel()

            await subgraph_queue.join()  # Wait for all subgraphs to be processed

            merge_worker.cancel()

            await asyncio.gather(*subgraph_workers, merge_worker)

            file_bar.close()
            with chunk_bar_lock:
                chunk_bar.close()
            subgraph_bar.close()
        except KeyboardInterrupt:
            executor.shutdown(wait=False, cancel_futures=True)

    return main_graph


def main(args):
    """Main function to handle command line arguments and process the input file."""
    # Parse command line arguments
    input_file = args.input_file
    entity_type = Path(args.input_file).stem  # Get entity type from filename

    # Configure output directory
    output_folder = Path(args.output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)
    output_file = output_folder / f"{entity_type}.ttl"

    # Initialize namespaces
    namespaces = {
        "schema": SCHEMA,
        "wdt": WDT,
        "wd": WD,
        "mb": MB,
        "mbae": MBAE,
        "mbat": MBAT,
        "mbev": MBEV,
        "mbin": MBIN,
        "mbla": MBLA,
        "mbpl": MBPL,
        "mbrc": MBRC,
        "mbrg": MBRG,
        "mbrl": MBRL,
        "mbse": MBSE,
        "mbwo": MBWO,
    }

    main_graph = asyncio.run(get_final_graph(entity_type, input_file, namespaces))

    # Save the final result
    print(f"Saving RDF data to: {output_file}")
    with open(output_file, "wb") as f:
        main_graph.serialize(f, format="turtle", encoding="utf-8")
    print(f"Successfully saved RDF data to: {output_file}")
    main_graph.close()

    # Fully delete the store
    for root, dirs, files in os.walk("./store", topdown=False):
        for file in files:
            os.remove(os.path.join(root, file))
        for name in dirs:
            os.rmdir(os.path.join(root, name))
    os.rmdir("./store")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Convert MusicBrainz JSON data in a folder to RDF Turtle format."
    )
    parser.add_argument(
        "--input_folder",
        default="../../data/musicbrainz/raw/extracted_jsonl/mbdump",
        help="Path to the folder containing line-delimited MusicBrainz JSON files.",
    )
    parser.add_argument(
        "--output_folder",
        default="../../data/musicbrainz/rdf/",
        help="Directory where the output Turtle files will be saved.",
    )
    args = parser.parse_args()

    input_folder = Path(args.input_folder)
    if not input_folder.is_dir():
        print(f"{input_folder} is not a valid directory.")
        sys.exit(1)

    bad_files = []
    if Path(args.output_folder).exists() and not REPROCESSING:
        output_folder = Path(args.output_folder)
        for file in output_folder.iterdir():
            if file.is_file():
                bad_files.append(file.stem)

    for input_file in input_folder.iterdir():
        if str(input_file).endswith(".DS_Store"):
            continue
        if input_file.stem in bad_files and not REPROCESSING:
            print(f"Skipping {input_file} as it is already processed.")
            continue
        if input_file.is_file():
            print(f"Processing file: {input_file}")
            # Create a new namespace for the current file using its stem as entity type
            sub_args = argparse.Namespace(
                input_file=str(input_file), output_folder=args.output_folder
            )
            main(sub_args)
