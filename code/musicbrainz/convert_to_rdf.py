"""
Module: convert_to_rdf.py
This module converts line-delimited MusicBrainz JSON data into RDF Turtle format using the rdflib library.
It reads an input file where each line represents a MusicBrainz entity, processes the data to map various
properties to corresponding Wikidata properties, and serializes the resulting RDF graph to a Turtle file.

Key Features:

    - Reads and counts total lines in the input JSONL file.
    - Infers the entity type based on the filename of the input.
    - Processes entity attributes including name, type, aliases, genres, and relationships.
    - Uses a mapping schema (MB_SCHEMA) to convert MusicBrainz entity relationships to corresponding Wikidata properties.
    - Processes data in chunks and utilizes asynchronous workers and multiprocessing for efficient data handling.
    - Merges RDF subgraphs generated by worker threads into a main RDF graph.
    - Serializes the main RDF graph to an output Turtle (.ttl) file specified via command line.
    - Supports splitting the graph into multiple Turtle files if the input file is large.
    - Supports reading reconciled mappings for types and keys from a CSV file.
    - Provides progress feedback using tqdm for monitoring both file reading and graph merging processes.

Usage:
    python3 convert_to_rdf.py --input_folder <input_folder> --reconciled_folder <reconciled_folder>
        --config_folder <config_folder> --output_folder <output_folder>

    Where <input_folder> is a path to the folder containing line-delimited JSON files containing MusicBrainz data,
    <reconciled_folder> is the path to the folder containing reconciled mappings for types and keys,
    <config_folder> is the path to the folder containing configuration files for the schema and mappings,
    and <output_folder> is the path to the folder to save the generated Turtle files.
    The script can also be run without arguments, in which case it will use default paths.
    The input folder should contain files named according to the entity type (e.g., artist.jsonl, release.jsonl).
    The output folder will contain the generated Turtle files named after the entity type.
    The script will create the output folder if it does not exist.

Exception Handling:

    - Skips any lines that fail JSON decoding or lack the necessary entity id.
    - Handles missing or malformed data gracefully, logging errors to the terminal without crashing the script.
    - Unexpected exceptions that cause workers to stop are logged, the worker will mark the problematic task
    as complete so that other workers don't get stuck, and the worker will exit.

This module is intended for converting MusicBrainz data to RDF format, facilitating integration with other linked data sources.
"""

import json
import sys
import os
import re
import argparse
import asyncio
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from isodate.isoerror import ISO8601Error
from isodate.isodates import parse_date
from isodate.isodatetime import parse_datetime
from tqdm import tqdm
from rdflib import Graph, ConjunctiveGraph, URIRef, Literal, Namespace
from rdflib.namespace import XSD, RDF
import pandas as pd
import aiofiles
from mapping_schema import MappingSchema

# Define namespaces
WDT = Namespace("http://www.wikidata.org/prop/direct/")
WD = Namespace("http://www.wikidata.org/entity/")
# This namespace is to encode coordinates like Wikidata does
GEO = Namespace("http://www.opengis.net/ont/geosparql#")
LMMB = Namespace("https://linkedmusic.ca/graphs/musicbrainz/")
MB = Namespace("https://musicbrainz.org/")
# Define MusicBrainz namespaces, to save space in exported RDF
MBAE = Namespace(f"{MB}area/")
MBAT = Namespace(f"{MB}artist/")
MBCD = Namespace(f"{MB}cdtoc/")
MBEV = Namespace(f"{MB}event/")
MBGE = Namespace(f"{MB}genre/")
MBIN = Namespace(f"{MB}instrument/")
MBLA = Namespace(f"{MB}label/")
MBPL = Namespace(f"{MB}place/")
MBRC = Namespace(f"{MB}recording/")
MBRG = Namespace(f"{MB}release-group/")
MBRL = Namespace(f"{MB}release/")
MBSE = Namespace(f"{MB}series/")
MBWO = Namespace(f"{MB}work/")

MB_SCHEMA = MappingSchema({})

# Initialize the relationship and attribute mappings
RELATIONSHIP_MAPPING = {}
ATTRIBUTE_MAPPING = {}
RECONCILIATION_MAPPING = {}

CHUNK_SIZE = 500  # Adjustable chunk size
# Max number of chunk processing processes to run simultaneously
MAX_SIMULTANEOUS_CHUNK_WORKERS = 3
# Max number of subgraph merging threads to run simultaneously
MAX_SIMULTANEOUS_SUBGRAPH_WORKERS = 3
# Max number of graph serializing processes to run simultaneously
MAX_SIMULTANEOUS_GRAPH_WORKERS = 3
# Max number of processes to run simultaneously
MAX_PROCESSES = min(
    MAX_SIMULTANEOUS_CHUNK_WORKERS + MAX_SIMULTANEOUS_GRAPH_WORKERS, os.cpu_count() or 1
)
MAX_CHUNKS_IN_MEMORY = 120  # Max number of chunks to keep in memory at once
MAX_SUBGRAPHS_IN_MEMORY = 120  # Max number of subgraphs to keep in memory at once

# If the input file is bigger (in bytes) than this, it will use Oxigraph to store the graph
# Otherwise it will use rdflib's in-memory graph
GRAPH_STORE_CUTOFF = 1000000000
# Number of chunks after which a new graph will be created
# This will only be used if the input file is big enough to use Oxigraph
MAX_CHUNKS_PER_GRAPH = 2000

# Set to True if you want to reprocess entity types that are already present in the output folder
REPROCESSING = False

# List of statuses for works that are end causes
# The remaining statuses will be interpreted as a type of work
END_STATUSES = [
    "Cancelled",
    "Withdrawn",
    "Expunged",
]

# Entity types that do not have types, so we don't need to process them
ENTITIES_WITHOUT_TYPES = [
    "recording",
    "release",
]

# This one is separated because I need to convert from /wiki/... to /entity/...
WIKIDATA_REGEX = re.compile(r"^https?:\/\/www\.wikidata\.org\/wiki\/Q\d+$")


def matched_wikidata(field: str) -> bool:
    """Check if the field is a matched Wikidata ID."""
    return re.match(r"^Q\d+", field) is not None


def convert_date(date_str: str) -> Literal:
    """
    Convert a date string to an RDF Literal with XSD date datatype.
    If the date string is not in a valid format, it returns a plain Literal.
    """
    try:
        # Validate the date string, and catch any exception that might occur
        return Literal(parse_date(date_str), datatype=XSD.date)
    except (ISO8601Error, ValueError):
        return Literal(date_str)  # Fallback to a plain literal if conversion fails


def convert_datetime(date_str: str, time_str: str) -> Literal:
    """
    Convert a date and time string to an RDF Literal with XSD dateTime datatype.
    If the date or time string is not in a valid format, it returns a plain Literal.
    """
    try:
        # Validate the datetime string, and catch any exception that might occur
        return Literal(
            parse_datetime(f"{date_str}T{time_str}:00"), datatype=XSD.dateTime
        )
    except (ISO8601Error, ValueError):
        return Literal(date_str)  # Fallback to a plain literal if conversion fails


def dashes_to_upper_camel(string: str) -> str:
    """
    Convert a string with dashes to UpperCamelCase.
    Example: "release-group" -> "ReleaseGroup"
    """
    return "".join(word.capitalize() for word in string.split("-"))


def process_entity(
    data,
    entity_type,
    mb_schema,
    relationship_mapping,
    reconciled_mapping,
    attribute_mapping,
    g,
):
    """Process a single line of JSON data and add it to the RDF graph."""
    entity_id = data.get("id")
    if not entity_id:
        return

    entity_mb_schema = mb_schema.to_dict_for_type(entity_type)
    entity_relationship_mapping = relationship_mapping.get(entity_type, {})

    # Create subject URI
    subject_uri = URIRef(f"https://musicbrainz.org/{entity_type}/{entity_id}")

    # Add the entity type, use UpperCamelCase for entity type
    g.add((subject_uri, RDF.type, LMMB[dashes_to_upper_camel(entity_type)]))

    # Process name
    if name := data.get("name"):
        g.add((subject_uri, entity_mb_schema["name"], Literal(name)))

    # Process type
    for t in data.get("secondary-types", []) + [
        data.get("primary-type"),
        data.get("type"),
    ]:
        if t and (converted_type := reconciled_mapping.get(t)):
            # If the type is a Wikidata ID, use it directly
            if matched_wikidata(converted_type):
                g.add(
                    (
                        subject_uri,
                        entity_mb_schema["type"],
                        URIRef(f"{WD}{converted_type}"),
                    )
                )
            else:
                g.add((subject_uri, entity_mb_schema["type"], Literal(t)))

    # Process address
    if address := data.get("address"):
        g.add(
            (
                subject_uri,
                entity_mb_schema["address"],
                Literal(address),
            )
        )

    # Process aliases
    for alias in data.get("aliases", []):
        if alias_name := alias.get("name"):
            try:
                g.add(
                    (
                        subject_uri,
                        entity_mb_schema["alias"],
                        Literal(
                            alias_name,
                            lang=alias.get("locale", "none"),
                        ),
                    )
                )
            except ValueError:  # If the language isn't in the standard RDF languages
                g.add(
                    (
                        subject_uri,
                        entity_mb_schema["alias"],
                        Literal(alias_name),
                    )
                )

    # Process area
    if area := data.get("area"):
        if area_id := area.get("id"):
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["area"],
                    URIRef(f"https://musicbrainz.org/area/{area_id}"),
                )
            )

    # Process artists
    for artist in data.get("artist-credit", []):
        if artist_id := artist.get("artist", {}).get("id"):
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["artist"],
                    URIRef(f"https://musicbrainz.org/artist/{artist_id}"),
                )
            )

    # Process attributes
    for attribute in data.get("attributes", []):
        if (attribute_type := attribute.get("type")) and (
            attribute_value := attribute.get("value")
        ):
            if pred := attribute_mapping.get(attribute_type):
                if (
                    attribute_type == "Key"
                    and (key_map := reconciled_mapping.get(attribute_value))
                    and matched_wikidata(key_map)
                ):
                    # If the attribute is the key and it is a Wikidata ID, use it directly
                    attribute_value = URIRef(f"{WD}{key_map}")
                else:
                    attribute_value = Literal(attribute_value)

                g.add(
                    (
                        subject_uri,
                        pred,
                        attribute_value,
                    )
                )

    # Process begin area
    if begin_area := data.get("begin-area"):
        if begin_area_id := begin_area.get("id"):
            g.add(
                (
                    subject_uri,
                    (
                        entity_mb_schema["begin-area-person"]
                        if data["type"] == "Person"
                        else entity_mb_schema["begin-area"]
                    ),
                    URIRef(f"https://musicbrainz.org/area/{begin_area_id}"),
                )
            )

    # Process coordinates
    if coordinates := data.get("coordinates"):
        if (lat := coordinates.get("latitude")) and (
            lon := coordinates.get("longitude")
        ):
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["coordinates"],
                    Literal(f"Point({lon} {lat})", datatype=GEO["wktLiteral"]),
                )
            )

    # Process date
    if date := data.get("date"):
        g.add((subject_uri, entity_mb_schema["date"], convert_date(date)))

    # Process end area
    if end_area := data.get("end-area"):
        if (end_area_id := end_area.get("id")) and data["type"] == "Person":
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["end-area-person"],
                    URIRef(f"https://musicbrainz.org/area/{end_area_id}"),
                )
            )

    # Process first release date
    if first_release_date := data.get("first-release-date"):
        g.add(
            (
                subject_uri,
                entity_mb_schema["first-release-date"],
                convert_date(first_release_date),
            )
        )

    # Process gender
    if gender := data.get("gender"):
        if (gender_map := reconciled_mapping.get(gender)) and matched_wikidata(
            gender_map
        ):
            gender = URIRef(f"{WD}{gender_map}")
        else:
            gender = Literal(gender)
        g.add(
            (
                subject_uri,
                entity_mb_schema["gender"],
                gender,
            )
        )

    # Process genres
    for genre in data.get("genres", []):
        if genre_id := genre.get("id"):
            genre_uri = URIRef(f"https://musicbrainz.org/genre/{genre_id}")
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["genre"],
                    genre_uri,
                )
            )

    # Process labels
    for label in data.get("label-info", []):
        label = label.get("label")
        if label and (label_id := label.get("id")):
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["label"],
                    URIRef(f"https://musicbrainz.org/label/{label_id}"),
                )
            )

    # Process languages
    for lang in data.get("languages", []):
        if lang_map := reconciled_mapping.get(lang):
            if matched_wikidata(lang_map):
                lang = URIRef(f"{WD}{lang_map}")
            else:
                lang = Literal(lang_map)
        else:
            lang = Literal(lang)
        g.add(
            (
                subject_uri,
                entity_mb_schema["language"],
                lang,
            )
        )

    # Process length
    if length := data.get("length"):
        try:
            length_seconds = int(length) // 1000  # Convert milliseconds to seconds
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["length"],
                    Literal(str(length_seconds), datatype=XSD.decimal),
                )
            )
        except (ValueError, TypeError):
            # Fallback to encoding as a regular string if conversion fails
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["length"],
                    Literal(length),
                )
            )

    # Process lifespan
    if lifespan := data.get("life-span"):
        if begin_date := lifespan.get("begin"):
            g.add(
                (
                    subject_uri,
                    (
                        entity_mb_schema["begin-date-person"]
                        if data["type"] == "Person"
                        else entity_mb_schema["begin-date"]
                    ),
                    convert_date(begin_date),
                )
            )
        if end_date := lifespan.get("end"):
            g.add(
                (
                    subject_uri,
                    (
                        entity_mb_schema["end-date-person"]
                        if data["type"] == "Person"
                        else entity_mb_schema["end-date"]
                    ),
                    convert_date(end_date),
                )
            )

    # Process media
    for media in data.get("media", []):
        for disc in media.get("discs", []):
            if disc_id := disc.get("id"):
                g.add(
                    (
                        subject_uri,
                        entity_mb_schema["cdtoc"],
                        URIRef(f"https://musicbrainz.org/cdtoc/{disc_id}"),
                    )
                )

        for track in media.get("tracks", []):
            if (recording := track.get("recording")) and (
                recording_id := recording.get("id")
            ):
                g.add(
                    (
                        subject_uri,
                        entity_mb_schema["recording"],
                        URIRef(f"https://musicbrainz.org/recording/{recording_id}"),
                    )
                )
                process_entity(
                    recording,
                    "recording",
                    mb_schema,
                    relationship_mapping,
                    reconciled_mapping,
                    attribute_mapping,
                    g,
                )

    # Process packaging
    if packaging := data.get("packaging"):
        if (packaging_map := reconciled_mapping.get(packaging)) and matched_wikidata(
            packaging_map
        ):
            packaging = URIRef(f"{WD}{packaging_map}")
        else:
            packaging = Literal(packaging)
        g.add(
            (
                subject_uri,
                entity_mb_schema["packaging"],
                packaging,
            )
        )

    # Process relationships
    for relation in data.get("relations", []):
        if not (target_type := relation.get("target-type")) or not (
            rel_type := relation.get("type")
        ):
            continue

        target = None
        pred_uri = None
        if target_type == "url" and (url := relation.get("url", {}).get("resource")):
            pred_uri = entity_mb_schema["url"]
            if WIKIDATA_REGEX.match(url):
                # Convert Wikidata URL to URIRef
                target = URIRef(
                    re.sub(
                        r"^https?:\/\/www\.wikidata\.org\/wiki\/(Q\d+)",
                        f"{WD}\\g<1>",
                        url,
                    )
                )
            else:
                # Treat it as a generic URL
                target = Literal(url)

        target_type = target_type.replace("_", "-")  # Normalize target type
        if not target:
            # Handle homogeneous relations
            if target_type == entity_type and (
                rel_direction := relation.get("direction")
            ):
                rel_type += f"_{rel_direction}"
            pred_uri = entity_relationship_mapping.get(target_type, {}).get(rel_type)

            # We need the underscores because the release group field will be `release_group`
            if target_id := relation.get(target_type.replace("-", "_"), {}).get("id"):
                # If the target is a MusicBrainz entity, create a URIRef
                target = URIRef(f"{MB}{target_type}/{target_id}")

        # Handle instrument relationships
        if (
            entity_type == "recording"
            and target_type == "artist"
            and rel_type == "instrument"
        ):
            for inst in relation.get("attribute-ids", {}).values():
                g.add(
                    (
                        subject_uri,
                        entity_mb_schema["instrument"],
                        MBIN[inst],
                    )
                )

        if target and pred_uri:
            # All genre relationships need the genre as a subject
            if target_type == "genre":
                g.add((target, pred_uri, subject_uri))
            else:
                g.add((subject_uri, pred_uri, target))

    # Process release events
    for event in data.get("release-events", []):
        area = event.get("area")
        if area and (area_id := area.get("id")):
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["area"],
                    URIRef(f"https://musicbrainz.org/area/{area_id}"),
                )
            )

    # Process release group
    if release_group := data.get("release-group"):
        if release_group_id := release_group.get("id"):
            g.add(
                (
                    subject_uri,
                    entity_mb_schema["release-group"],
                    URIRef(f"https://musicbrainz.org/release-group/{release_group_id}"),
                )
            )

    # Process status
    if status := data.get("status"):
        if (status_map := reconciled_mapping.get(status)) and matched_wikidata(
            status_map
        ):
            status_rdf = URIRef(f"{WD}{status_map}")
        else:
            status_rdf = Literal(status)
        g.add(
            (
                subject_uri,
                (
                    entity_mb_schema["status"]
                    if status not in END_STATUSES
                    else entity_mb_schema["end-status"]
                ),
                status_rdf,
            )
        )

    # Process time
    if (time := data.get("time")) and (date := data.get("life-span", {}).get("begin")):
        # If both date and time are present, convert them to a datetime literal
        g.add(
            (
                subject_uri,
                entity_mb_schema["time"],
                convert_datetime(date, time),
            )
        )

    # Process title
    if title := data.get("title"):
        g.add((subject_uri, entity_mb_schema["title"], Literal(title)))


def process_chunk(
    chunk,
    entity_type,
    mb_schema,
    relationship_mapping,
    reconciled_mapping,
    attribute_mapping,
):
    """Process a chunk of data and add it to the subgraph."""
    g = Graph()
    for i, line in enumerate(chunk):
        try:
            data = json.loads(line.strip())
            process_entity(
                data,
                entity_type,
                mb_schema,
                relationship_mapping,
                reconciled_mapping,
                attribute_mapping,
                g,
            )
        except json.JSONDecodeError:
            continue
        except (KeyError, AttributeError) as e:
            with tqdm.get_lock():
                tqdm.write(f"{type(e).__name__} in line {i} of chunk: {e}")
            continue
        except Exception as e:
            with tqdm.get_lock():
                tqdm.write(f"Unexpected {type(e).__name__} in line {i} of chunk: {e}")
            continue
        finally:
            chunk[i] = None  # Clear the processed line to free memory
    return g


async def chunk_worker(
    chunk_queue,
    subgraph_queue,
    entity_type,
    mb_schema,
    relationship_mapping,
    reconciled_mapping,
    chunk_bar,
    executor,
):
    """
    Worker function to process data chunks.
    This function runs in a separate process to speed up the processing.
    """
    loop = asyncio.get_event_loop()
    chunk_started = False
    try:
        while True:
            chunk = await chunk_queue.get()
            chunk_started = True

            # Process the chunk in a separate process to speed up the processing
            g = await asyncio.gather(
                loop.run_in_executor(
                    executor,
                    process_chunk,
                    chunk,
                    entity_type,
                    mb_schema,
                    relationship_mapping,
                    reconciled_mapping,
                    ATTRIBUTE_MAPPING,
                ),
                return_exceptions=True,
            )
            g = g[0]

            # Handle any exceptions raised by the process
            if isinstance(g, Exception):
                chunk_queue.task_done()
                with tqdm.get_lock():
                    tqdm.write(f"Error processing chunk: {type(g).__name__}: {g}")
                    chunk_bar.update(1)
                chunk_started = False
                continue

            await subgraph_queue.put(g)  # Add the subgraph to the queue
            chunk_queue.task_done()
            with tqdm.get_lock():
                chunk_bar.update(1)
            chunk_started = False
    except asyncio.CancelledError:
        pass
    except Exception as e:
        with tqdm.get_lock():
            tqdm.write(f"Error processing chunk: {type(e).__name__}: {e}")
    finally:
        # Ensure all tasks are marked as done
        if chunk_started:
            chunk_queue.task_done()
            # If the chunk was not processed, we still need to update the progress bar
            with tqdm.get_lock():
                chunk_bar.update(1)


def merge_subgraph(graph, subgraph):
    """Merge a subgraph into the main graph."""
    for s, p, o in subgraph:
        try:
            graph.add((s, p, o))
        except Exception as e:
            with tqdm.get_lock():
                tqdm.write(
                    f"Error adding triple ({s}, {p}, {o}) to main graph: {type(e).__name__}: {e}"
                )
            continue
    graph.commit()  # Commit the changes to the main graph


async def merge_worker(
    subgraph_queue, graph_queue, graph_number_queue, graph_store, subgraph_bar
):
    """Worker function to merge subgraphs into the main graph."""
    graph_num = graph_number_queue.get_nowait()
    if graph_store:
        graph = Graph("Oxigraph")
        graph.open(f"./store-{graph_num}", create=True)
    else:
        graph = Graph()

    chunk_count = 0
    loop = asyncio.get_event_loop()
    chunk_started = False
    try:
        while True:
            subgraph = await subgraph_queue.get()
            chunk_started = True

            # Do this in a separate thread to avoid blocking the event loop
            await loop.run_in_executor(None, merge_subgraph, graph, subgraph)

            subgraph_queue.task_done()
            with tqdm.get_lock():
                subgraph_bar.update(1)
            chunk_started = False
            chunk_count += 1

            if graph_store and chunk_count % MAX_CHUNKS_PER_GRAPH == 0:
                try:
                    new_graph_num = graph_number_queue.get_nowait()
                except asyncio.QueueEmpty:
                    break  # No more graphs to create
                # Save the current graph to the queue
                if graph_store:
                    graph.commit()
                    graph.close()
                    await graph_queue.put((graph_num, f"./store-{graph_num}"))
                else:
                    await graph_queue.put((graph_num, graph))
                # Start a new graph
                graph_num = new_graph_num
                graph = Graph("Oxigraph")
                graph.open(f"./store-{graph_num}", create=True)
    except asyncio.CancelledError:
        pass
    except Exception as e:
        with tqdm.get_lock():
            tqdm.write(f"Error merging subgraph: {type(e).__name__}: {e}")
    finally:
        # Ensure all tasks are marked as done
        if chunk_started:
            subgraph_queue.task_done()
            # If the last subgraph was not processed, we still need to update the progress bar
            with tqdm.get_lock():
                subgraph_bar.update(1)
        if graph_store:
            graph.commit()
            graph.close()
            await graph_queue.put((graph_num, f"./store-{graph_num}"))
        else:
            await graph_queue.put((graph_num, graph))


def serialize_graph(graph, filename, namespaces):
    """
    Serialize the graph to a Turtle file.
    If a string is given, it is assumed to be the path to the store for the graph.
    """
    if isinstance(graph, Graph):
        for prefix, ns in namespaces.items():
            graph.bind(prefix, ns)
        with open(filename, "wb") as f:
            graph.serialize(f, format="turtle", encoding="utf-8")
    else:
        # Use a ConjunctiveGraph to get around the fact that Oxigraph is a quad store
        # and rdflib's Graph is a triple store, which can cause issues with reopening the graph
        # due to triples not appearing
        g = ConjunctiveGraph("Oxigraph")
        g.open(graph, create=False)
        for prefix, ns in namespaces.items():
            g.bind(prefix, ns)
        with open(filename, "wb") as f:
            g.serialize(f, format="turtle", encoding="utf-8")


async def serialize_worker(
    entity_type, output_folder, graph_queue, serialize_bar, namespaces, executor
):
    """
    Worker function to serialize graphs.
    This function runs in a separate process to speed up the serialization.
    """
    loop = asyncio.get_event_loop()
    graph_started = False
    try:
        while True:
            i, graph = await graph_queue.get()
            graph_started = True
            output_file = output_folder / f"{entity_type}-{i}.ttl"
            await loop.run_in_executor(
                executor, serialize_graph, graph, str(output_file), namespaces
            )
            # Fully delete the stores
            if os.path.exists(f"./store-{i}"):
                for root, dirs, files in os.walk(f"./store-{i}", topdown=False):
                    for file in files:
                        os.remove(os.path.join(root, file))
                    for name in dirs:
                        os.rmdir(os.path.join(root, name))
                os.rmdir(f"./store-{i}")
            graph_queue.task_done()
            with tqdm.get_lock():
                serialize_bar.update(1)
            graph_started = False
    except asyncio.CancelledError:
        pass
    except Exception as e:
        with tqdm.get_lock():
            tqdm.write(f"Error serializing graph: {type(e).__name__}: {e}")
    finally:
        # Ensure all tasks are marked as done
        if graph_started:
            graph_queue.task_done()
            # If the graph was not processed, we still need to update the progress bar
            with tqdm.get_lock():
                serialize_bar.update(1)


async def create_graphs(
    entity_type, input_file, output_folder, namespaces, reconciled_mapping
):
    """Main function to process the input file and export the final RDF graphs."""
    with open(input_file, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)
    total_chunks = (
        total_lines // CHUNK_SIZE + 1
        if total_lines % CHUNK_SIZE != 0
        else total_lines // CHUNK_SIZE
    )
    print(f"Total lines in {input_file}: {total_lines}")
    print(f"Total number of chunks: {total_chunks}")
    print(f"Processing {input_file}...")

    if os.path.getsize(input_file) > GRAPH_STORE_CUTOFF:
        graph_store = True
    else:
        graph_store = False
        print(f"{input_file} is small enough to use an in-memory graph.")

    # Only use 1 graph if we don't use a graph store
    if graph_store:
        graph_count = (
            total_chunks // MAX_CHUNKS_PER_GRAPH + 1
            if total_chunks % MAX_CHUNKS_PER_GRAPH != 0
            else total_chunks // MAX_CHUNKS_PER_GRAPH
        )
    else:
        graph_count = 1

    # Create queues
    chunk_queue = asyncio.Queue(MAX_CHUNKS_IN_MEMORY)
    subgraph_queue = asyncio.Queue(MAX_SUBGRAPHS_IN_MEMORY)
    graph_queue = asyncio.Queue(graph_count)
    # This queue is to ensure that graph numbering is consistent across all workers
    # It also allows workers to know when to stop creating graphs
    graph_number_queue = asyncio.Queue(graph_count)
    for i in range(graph_count):
        graph_number_queue.put_nowait(i)

    # Create the progress bars
    file_bar = tqdm(total=total_lines, desc="Processing lines", position=0)
    chunk_bar = tqdm(total=total_chunks, desc="Processing chunks", position=1)
    subgraph_bar = tqdm(total=total_chunks, desc="Merging subgraphs", position=2)
    serialize_bar = tqdm(total=graph_count, desc="Saving RDF graphs", position=3)

    with ProcessPoolExecutor(max_workers=MAX_PROCESSES) as executor:
        try:
            subgraph_workers = [
                asyncio.create_task(
                    chunk_worker(
                        chunk_queue,
                        subgraph_queue,
                        entity_type,
                        MB_SCHEMA,
                        RELATIONSHIP_MAPPING,
                        reconciled_mapping,
                        chunk_bar,
                        executor,
                    )
                )
                for _ in range(MAX_SIMULTANEOUS_CHUNK_WORKERS)
            ]
            merge_workers = [
                asyncio.create_task(
                    merge_worker(
                        subgraph_queue,
                        graph_queue,
                        graph_number_queue,
                        graph_store,
                        subgraph_bar,
                    )
                )
                for _ in range(min(graph_count, MAX_SIMULTANEOUS_SUBGRAPH_WORKERS))
            ]
            serialize_workers = [
                asyncio.create_task(
                    serialize_worker(
                        entity_type,
                        output_folder,
                        graph_queue,
                        serialize_bar,
                        namespaces,
                        executor,
                    )
                )
                for _ in range(min(graph_count, MAX_SIMULTANEOUS_GRAPH_WORKERS))
            ]

            # Read file and split into chunks
            async with aiofiles.open(input_file, "r", encoding="utf-8") as f:
                chunk = []
                async for line in f:
                    chunk.append(line)
                    if len(chunk) >= CHUNK_SIZE:
                        await chunk_queue.put(chunk)
                        chunk = []
                    with tqdm.get_lock():
                        file_bar.update(1)
                if chunk:  # Process the remaining last chunk
                    await chunk_queue.put(chunk)

            with tqdm.get_lock():
                file_bar.refresh()

            await chunk_queue.join()  # Wait for all chunks to be processed

            for worker in subgraph_workers:
                worker.cancel()

            await asyncio.gather(*subgraph_workers)

            with tqdm.get_lock():
                chunk_bar.refresh()

            await subgraph_queue.join()  # Wait for all subgraphs to be processed

            for worker in merge_workers:
                worker.cancel()

            await asyncio.gather(*merge_workers)

            with tqdm.get_lock():
                subgraph_bar.refresh()

            await graph_queue.join()  # Wait for all graphs to be serialized

            for worker in serialize_workers:
                worker.cancel()

            await asyncio.gather(*serialize_workers)

            file_bar.close()
            chunk_bar.close()
            subgraph_bar.close()
            serialize_bar.close()
        except KeyboardInterrupt:
            executor.shutdown(wait=False, cancel_futures=True)


def main(args):
    """Main function to handle command line arguments and process the input file."""
    # Parse command line arguments
    input_file = args.input_file
    entity_type = Path(input_file).stem  # Get entity type from filename
    type_file = Path(args.type_file) if args.type_file else None
    if type_file:
        # Read the types from the type file
        types = pd.read_csv(type_file, encoding="utf-8")
        reconciled_mapping = dict(zip(types["type"], types["type_@id"]))
    else:
        reconciled_mapping = {}
    reconciled_mapping.update(RECONCILIATION_MAPPING)

    # Configure output directory
    output_folder = Path(args.output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)

    # Initialize namespaces
    namespaces = {
        "wdt": WDT,
        "wd": WD,
        "geo": GEO,
        "mb": LMMB,
        "mbae": MBAE,
        "mbat": MBAT,
        "mbcd": MBCD,
        "mbev": MBEV,
        "mbge": MBGE,
        "mbin": MBIN,
        "mbla": MBLA,
        "mbpl": MBPL,
        "mbrc": MBRC,
        "mbrg": MBRG,
        "mbrl": MBRL,
        "mbse": MBSE,
        "mbwo": MBWO,
    }

    asyncio.run(
        create_graphs(
            entity_type,
            input_file,
            output_folder,
            namespaces,
            reconciled_mapping,
        )
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Convert MusicBrainz JSON data in a folder to RDF Turtle format."
    )
    parser.add_argument(
        "--input_folder",
        default="../../data/musicbrainz/raw/extracted_jsonl/mbdump/",
        help="Path to the folder containing line-delimited MusicBrainz JSON files.",
    )
    parser.add_argument(
        "--reconciled_folder",
        default="../../data/musicbrainz/raw/reconciled/",
        help="Path to the folder containing data reconciled against Wikidata.",
    )
    parser.add_argument(
        "--config_folder",
        default="./rdf_conversion_config/",
        help="Path to the folder containing MusicBrainz RDF conversion configuration files (the property, relationship, and attribute mapping files).",
    )
    parser.add_argument(
        "--output_folder",
        default="../../data/musicbrainz/rdf/",
        help="Directory where the output Turtle files will be saved.",
    )
    args = parser.parse_args()

    input_folder = Path(args.input_folder)
    if not input_folder.is_dir():
        print(f"{input_folder} is not a valid directory.")
        sys.exit(1)

    config_folder = Path(args.config_folder)
    if not config_folder.is_dir():
        print(f"{config_folder} is not a valid directory.")
        sys.exit(1)

    with open(config_folder / "mappings.json", "r", encoding="utf-8") as fi:
        MB_SCHEMA.add_from_formatted_dict(json.load(fi))
    if not MB_SCHEMA:
        print("No mappings found in the configuration file.")
        sys.exit(1)

    with open(config_folder / "attribute_mapping.json", "r", encoding="utf-8") as fi:
        ATTRIBUTE_MAPPING = json.load(fi)
    if not ATTRIBUTE_MAPPING:
        print("No attribute mapping found in the configuration file.")
        sys.exit(1)

    for k, v in ATTRIBUTE_MAPPING.items():
        ATTRIBUTE_MAPPING[k] = URIRef(f"{WDT}{v}") if v else None

    with open(config_folder / "relations.json", "r", encoding="utf-8") as fi:
        RELATIONSHIP_MAPPING = json.load(fi)
    if not RELATIONSHIP_MAPPING:
        print("No relationship mapping found in the configuration file.")
        sys.exit(1)

    # Convert the property ID strings into URIRefs
    for mapping in RELATIONSHIP_MAPPING.values():
        if not mapping:
            continue
        for values in mapping.values():
            if not values:
                continue
            for k, v in values.items():
                if v is not None:
                    values[k] = WDT[v]

    keys_file_path = Path(args.reconciled_folder) / "keys-csv.csv"
    if keys_file_path.is_file():
        with open(keys_file_path, "r", encoding="utf-8") as fi:
            keys = pd.read_csv(fi, encoding="utf-8")
            RECONCILIATION_MAPPING.update(dict(zip(keys["key"], keys["key_@id"])))

    genders_file_path = Path(args.reconciled_folder) / "genders-csv.csv"
    if genders_file_path.is_file():
        with open(genders_file_path, "r", encoding="utf-8") as fi:
            genders = pd.read_csv(fi, encoding="utf-8")
            RECONCILIATION_MAPPING.update(
                dict(zip(genders["gender"], genders["gender_@id"]))
            )

    languages_file_path = Path(args.reconciled_folder) / "languages-csv.csv"
    if languages_file_path.is_file():
        with open(languages_file_path, "r", encoding="utf-8") as fi:
            languages = pd.read_csv(fi, encoding="utf-8")
            RECONCILIATION_MAPPING.update(
                dict(zip(languages["language"], languages["full_language_@id"]))
            )

    packagings_file_path = Path(args.reconciled_folder) / "packagings-csv.csv"
    if packagings_file_path.is_file():
        with open(packagings_file_path, "r", encoding="utf-8") as fi:
            packagings = pd.read_csv(fi, encoding="utf-8")
            RECONCILIATION_MAPPING.update(
                dict(zip(packagings["packaging"], packagings["packaging_@id"]))
            )

    statuses_file_path = Path(args.reconciled_folder) / "statuses-csv.csv"
    if statuses_file_path.is_file():
        with open(statuses_file_path, "r", encoding="utf-8") as fi:
            statuses = pd.read_csv(fi, encoding="utf-8")
            RECONCILIATION_MAPPING.update(
                dict(zip(statuses["status"], statuses["status_@id"]))
            )

    bad_files = set()
    if Path(args.output_folder).exists() and not REPROCESSING:
        output_folder = Path(args.output_folder)
        for file in output_folder.iterdir():
            # Get rid of numbers for ttl files
            if file.is_file() and (match := re.match(r"^([\w-]+)-\d+$", file.stem)):
                bad_files.add(match.group(1))

    for input_file in input_folder.iterdir():
        if not input_file.is_file() or not str(input_file).endswith(".jsonl"):
            continue
        if not REPROCESSING and input_file.stem in bad_files:
            print(f"Skipping {input_file} as it is already processed.")
            continue
        type_file = None
        if input_file.stem not in ENTITIES_WITHOUT_TYPES:
            type_file_path = (
                Path(args.reconciled_folder) / f"{input_file.stem}-types-csv.csv"
            )
            if type_file_path.exists() and type_file_path.is_file():
                type_file = str(type_file_path)
        print(f"Processing file: {input_file}")

        # Create a new namespace for the current file using its stem as entity type
        sub_args = argparse.Namespace(
            input_file=str(input_file),
            type_file=type_file,
            output_folder=args.output_folder,
        )
        main(sub_args)
